{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 23114,
     "status": "ok",
     "timestamp": 1643031596124,
     "user": {
      "displayName": "Quỳnh Tôn",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "17019808029748888744"
     },
     "user_tz": -420
    },
    "id": "cwoQke3380TY",
    "outputId": "b4f081eb-aaa1-49b4-98fb-9b227d3d0764"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark-2.4.0-bin-hadoop2.7/\n",
      "spark-2.4.0-bin-hadoop2.7/python/\n",
      "spark-2.4.0-bin-hadoop2.7/python/setup.cfg\n",
      "spark-2.4.0-bin-hadoop2.7/python/pyspark/\n",
      "spark-2.4.0-bin-hadoop2.7/python/pyspark/resultiterable.py\n",
      "spark-2.4.0-bin-hadoop2.7/python/pyspark/python/\n",
      "spark-2.4.0-bin-hadoop2.7/python/pyspark/python/pyspark/\n",
      "spark-2.4.0-bin-hadoop2.7/python/pyspark/python/pyspark/shell.py\n",
      "spark-2.4.0-bin-hadoop2.7/python/pyspark/heapq3.py\n",
      "spark-2.4.0-bin-hadoop2.7/python/pyspark/join.py\n",
      "spark-2.4.0-bin-hadoop2.7/python/pyspark/version.py\n",
      "spark-2.4.0-bin-hadoop2.7/python/pyspark/rdd.py\n",
      "spark-2.4.0-bin-hadoop2.7/python/pyspark/java_gateway.py\n",
      "spark-2.4.0-bin-hadoop2.7/python/pyspark/find_spark_home.py\n",
      "spark-2.4.0-bin-hadoop2.7/python/pyspark/_globals.py\n",
      "spark-2.4.0-bin-hadoop2.7/python/pyspark/worker.py\n",
      "spark-2.4.0-bin-hadoop2.7/python/pyspark/accumulators.py\n",
      "spark-2.4.0-bin-hadoop2.7/python/pyspark/mllib/\n",
      "spark-2.4.0-bin-hadoop2.7/python/pyspark/mllib/feature.py\n",
      "spark-2.4.0-bin-hadoop2.7/python/pyspark/mllib/random.py\n",
      "spark-2.4.0-bin-hadoop2.7/python/pyspark/mllib/recommendation.py\n",
      "spark-2.4.0-bin-hadoop2.7/python/pyspark/mllib/fpm.py\n",
      "spark-2.4.0-bin-hadoop2.7/python/pyspark/mllib/classification.py\n",
      "spark-2.4.0-bin-hadoop2.7/python/pyspark/mllib/stat/\n",
      "spark-2.4.0-bin-hadoop2.7/python/pyspark/mllib/stat/KernelDensity.py\n",
      "spark-2.4.0-bin-hadoop2.7/python/pyspark/mllib/stat/distribution.py\n",
      "spark-2.4.0-bin-hadoop2.7/python/pyspark/mllib/stat/test.py\n",
      "spark-2.4.0-bin-hadoop2.7/python/pyspark/mllib/stat/_statistics.py\n",
      "spark-2.4.0-bin-hadoop2.7/python/pyspark/mllib/stat/__init__.py\n",
      "spark-2.4.0-bin-hadoop2.7/python/pyspark/mllib/util.py\n",
      "spark-2.4.0-bin-hadoop2.7/python/pyspark/mllib/regression.py\n",
      "spark-2.4.0-bin-hadoop2.7/python/pyspark/mllib/clustering.py\n",
      "spark-2.4.0-bin-hadoop2.7/python/pyspark/mllib/linalg/\n",
      "spark-2.4.0-bin-hadoop2.7/python/pyspark/mllib/linalg/distributed.py\n",
      "spark-2.4.0-bin-hadoop2.7/python/pyspark/mllib/linalg/__init__.py\n",
      "spark-2.4.0-bin-hadoop2.7/python/pyspark/mllib/tree.py\n",
      "spark-2.4.0-bin-hadoop2.7/python/pyspark/mllib/__init__.py\n",
      "spark-2.4.0-bin-hadoop2.7/python/pyspark/mllib/evaluation.py\n",
      "spark-2.4.0-bin-hadoop2.7/python/pyspark/mllib/common.py\n",
      "spark-2.4.0-bin-hadoop2.7/python/pyspark/mllib/tests.py\n",
      "spark-2.4.0-bin-hadoop2.7/python/pyspark/util.py\n",
      "spark-2.4.0-bin-hadoop2.7/python/pyspark/test_broadcast.py\n",
      "spark-2.4.0-bin-hadoop2.7/python/pyspark/shell.py\n",
      "spark-2.4.0-bin-hadoop2.7/python/pyspark/ml/\n",
      "spark-2.4.0-bin-hadoop2.7/python/pyspark/ml/feature.py\n",
      "spark-2.4.0-bin-hadoop2.7/python/pyspark/ml/recommendation.py\n",
      "spark-2.4.0-bin-hadoop2.7/python/pyspark/ml/fpm.py\n",
      "spark-2.4.0-bin-hadoop2.7/python/pyspark/ml/classification.py\n",
      "spark-2.4.0-bin-hadoop2.7/python/pyspark/ml/stat.py\n",
      "spark-2.4.0-bin-hadoop2.7/python/pyspark/ml/util.py\n",
      "spark-2.4.0-bin-hadoop2.7/python/pyspark/ml/tuning.py\n",
      "spark-2.4.0-bin-hadoop2.7/python/pyspark/ml/regression.py\n",
      "spark-2.4.0-bin-hadoop2.7/python/pyspark/ml/base.py\n",
      "spark-2.4.0-bin-hadoop2.7/python/pyspark/ml/clustering.py\n",
      "spark-2.4.0-bin-hadoop2.7/python/pyspark/ml/wrapper.py\n",
      "spark-2.4.0-bin-hadoop2.7/python/pyspark/ml/pipeline.py\n",
      "spark-2.4.0-bin-hadoop2.7/python/pyspark/ml/linalg/\n",
      "spark-2.4.0-bin-hadoop2.7/python/pyspark/ml/linalg/__init__.py\n",
      "spark-2.4.0-bin-hadoop2.7/python/pyspark/ml/param/\n",
      "spark-2.4.0-bin-hadoop2.7/python/pyspark/ml/param/_shared_params_code_gen.py\n",
      "spark-2.4.0-bin-hadoop2.7/python/pyspark/ml/param/shared.py\n",
      "spark-2.4.0-bin-hadoop2.7/python/pyspark/ml/param/__init__.py\n",
      "spark-2.4.0-bin-hadoop2.7/python/pyspark/ml/__init__.py\n",
      "spark-2.4.0-bin-hadoop2.7/python/pyspark/ml/evaluation.py\n",
      "spark-2.4.0-bin-hadoop2.7/python/pyspark/ml/common.py\n",
      "spark-2.4.0-bin-hadoop2.7/python/pyspark/ml/tests.py\n",
      "spark-2.4.0-bin-hadoop2.7/python/pyspark/ml/image.py\n",
      "spark-2.4.0-bin-hadoop2.7/python/pyspark/test_serializers.py\n",
      "spark-2.4.0-bin-hadoop2.7/python/pyspark/cloudpickle.py\n",
      "spark-2.4.0-bin-hadoop2.7/python/pyspark/serializers.py\n",
      "spark-2.4.0-bin-hadoop2.7/python/pyspark/daemon.py\n",
      "spark-2.4.0-bin-hadoop2.7/python/pyspark/files.py\n",
      "spark-2.4.0-bin-hadoop2.7/python/pyspark/rddsampler.py\n",
      "spark-2.4.0-bin-hadoop2.7/python/pyspark/statcounter.py\n",
      "spark-2.4.0-bin-hadoop2.7/python/pyspark/streaming/\n",
      "spark-2.4.0-bin-hadoop2.7/python/pyspark/streaming/kafka.py\n",
      "spark-2.4.0-bin-hadoop2.7/python/pyspark/streaming/util.py\n",
      "spark-2.4.0-bin-hadoop2.7/python/pyspark/streaming/kinesis.py\n",
      "spark-2.4.0-bin-hadoop2.7/python/pyspark/streaming/flume.py\n",
      "spark-2.4.0-bin-hadoop2.7/python/pyspark/streaming/__init__.py\n",
      "spark-2.4.0-bin-hadoop2.7/python/pyspark/streaming/context.py\n",
      "spark-2.4.0-bin-hadoop2.7/python/pyspark/streaming/tests.py\n",
      "spark-2.4.0-bin-hadoop2.7/python/pyspark/streaming/dstream.py\n",
      "spark-2.4.0-bin-hadoop2.7/python/pyspark/streaming/listener.py\n",
      "spark-2.4.0-bin-hadoop2.7/python/pyspark/taskcontext.py\n",
      "spark-2.4.0-bin-hadoop2.7/python/pyspark/__init__.py\n",
      "spark-2.4.0-bin-hadoop2.7/python/pyspark/storagelevel.py\n",
      "spark-2.4.0-bin-hadoop2.7/python/pyspark/broadcast.py\n",
      "spark-2.4.0-bin-hadoop2.7/python/pyspark/profiler.py\n",
      "spark-2.4.0-bin-hadoop2.7/python/pyspark/traceback_utils.py\n",
      "spark-2.4.0-bin-hadoop2.7/python/pyspark/context.py\n",
      "spark-2.4.0-bin-hadoop2.7/python/pyspark/status.py\n",
      "spark-2.4.0-bin-hadoop2.7/python/pyspark/sql/\n",
      "spark-2.4.0-bin-hadoop2.7/python/pyspark/sql/streaming.py\n",
      "spark-2.4.0-bin-hadoop2.7/python/pyspark/sql/readwriter.py\n",
      "spark-2.4.0-bin-hadoop2.7/python/pyspark/sql/catalog.py\n",
      "spark-2.4.0-bin-hadoop2.7/python/pyspark/sql/session.py\n",
      "spark-2.4.0-bin-hadoop2.7/python/pyspark/sql/column.py\n",
      "spark-2.4.0-bin-hadoop2.7/python/pyspark/sql/functions.py\n",
      "spark-2.4.0-bin-hadoop2.7/python/pyspark/sql/dataframe.py\n",
      "spark-2.4.0-bin-hadoop2.7/python/pyspark/sql/window.py\n",
      "spark-2.4.0-bin-hadoop2.7/python/pyspark/sql/udf.py\n",
      "spark-2.4.0-bin-hadoop2.7/python/pyspark/sql/__init__.py\n",
      "spark-2.4.0-bin-hadoop2.7/python/pyspark/sql/utils.py\n",
      "spark-2.4.0-bin-hadoop2.7/python/pyspark/sql/group.py\n",
      "spark-2.4.0-bin-hadoop2.7/python/pyspark/sql/context.py\n",
      "spark-2.4.0-bin-hadoop2.7/python/pyspark/sql/tests.py\n",
      "spark-2.4.0-bin-hadoop2.7/python/pyspark/sql/conf.py\n",
      "spark-2.4.0-bin-hadoop2.7/python/pyspark/sql/types.py\n",
      "spark-2.4.0-bin-hadoop2.7/python/pyspark/tests.py\n",
      "spark-2.4.0-bin-hadoop2.7/python/pyspark/conf.py\n",
      "spark-2.4.0-bin-hadoop2.7/python/pyspark/shuffle.py\n",
      "spark-2.4.0-bin-hadoop2.7/python/.gitignore\n",
      "spark-2.4.0-bin-hadoop2.7/python/docs/\n",
      "spark-2.4.0-bin-hadoop2.7/python/docs/epytext.py\n",
      "spark-2.4.0-bin-hadoop2.7/python/docs/make2.bat\n",
      "spark-2.4.0-bin-hadoop2.7/python/docs/pyspark.sql.rst\n",
      "spark-2.4.0-bin-hadoop2.7/python/docs/make.bat\n",
      "spark-2.4.0-bin-hadoop2.7/python/docs/index.rst\n",
      "spark-2.4.0-bin-hadoop2.7/python/docs/_templates/\n",
      "spark-2.4.0-bin-hadoop2.7/python/docs/_templates/layout.html\n",
      "spark-2.4.0-bin-hadoop2.7/python/docs/_static/\n",
      "spark-2.4.0-bin-hadoop2.7/python/docs/_static/pyspark.js\n",
      "spark-2.4.0-bin-hadoop2.7/python/docs/_static/pyspark.css\n",
      "spark-2.4.0-bin-hadoop2.7/python/docs/pyspark.ml.rst\n",
      "spark-2.4.0-bin-hadoop2.7/python/docs/pyspark.streaming.rst\n",
      "spark-2.4.0-bin-hadoop2.7/python/docs/conf.py\n",
      "spark-2.4.0-bin-hadoop2.7/python/docs/pyspark.mllib.rst\n",
      "spark-2.4.0-bin-hadoop2.7/python/docs/pyspark.rst\n",
      "spark-2.4.0-bin-hadoop2.7/python/docs/Makefile\n",
      "spark-2.4.0-bin-hadoop2.7/python/test_coverage/\n",
      "spark-2.4.0-bin-hadoop2.7/python/test_coverage/conf/\n",
      "spark-2.4.0-bin-hadoop2.7/python/test_coverage/conf/spark-defaults.conf\n",
      "spark-2.4.0-bin-hadoop2.7/python/test_coverage/sitecustomize.py\n",
      "spark-2.4.0-bin-hadoop2.7/python/test_coverage/coverage_daemon.py\n",
      "spark-2.4.0-bin-hadoop2.7/python/run-tests-with-coverage\n",
      "spark-2.4.0-bin-hadoop2.7/python/pyspark.egg-info/\n",
      "spark-2.4.0-bin-hadoop2.7/python/pyspark.egg-info/top_level.txt\n",
      "spark-2.4.0-bin-hadoop2.7/python/pyspark.egg-info/dependency_links.txt\n",
      "spark-2.4.0-bin-hadoop2.7/python/pyspark.egg-info/requires.txt\n",
      "spark-2.4.0-bin-hadoop2.7/python/pyspark.egg-info/PKG-INFO\n",
      "spark-2.4.0-bin-hadoop2.7/python/pyspark.egg-info/SOURCES.txt\n",
      "spark-2.4.0-bin-hadoop2.7/python/MANIFEST.in\n",
      "spark-2.4.0-bin-hadoop2.7/python/test_support/\n",
      "spark-2.4.0-bin-hadoop2.7/python/test_support/userlibrary.py\n",
      "spark-2.4.0-bin-hadoop2.7/python/test_support/userlib-0.1.zip\n",
      "spark-2.4.0-bin-hadoop2.7/python/test_support/SimpleHTTPServer.py\n",
      "spark-2.4.0-bin-hadoop2.7/python/test_support/hello/\n",
      "spark-2.4.0-bin-hadoop2.7/python/test_support/hello/sub_hello/\n",
      "spark-2.4.0-bin-hadoop2.7/python/test_support/hello/sub_hello/sub_hello.txt\n",
      "spark-2.4.0-bin-hadoop2.7/python/test_support/hello/hello.txt\n",
      "spark-2.4.0-bin-hadoop2.7/python/test_support/sql/\n",
      "spark-2.4.0-bin-hadoop2.7/python/test_support/sql/people_array.json\n",
      "spark-2.4.0-bin-hadoop2.7/python/test_support/sql/text-test.txt\n",
      "spark-2.4.0-bin-hadoop2.7/python/test_support/sql/ages_newlines.csv\n",
      "spark-2.4.0-bin-hadoop2.7/python/test_support/sql/people.json\n",
      "spark-2.4.0-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/\n",
      "spark-2.4.0-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2014/\n",
      "spark-2.4.0-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2014/month=9/\n",
      "spark-2.4.0-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2014/month=9/day=1/\n",
      "spark-2.4.0-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2014/month=9/day=1/part-r-00008.gz.parquet\n",
      "spark-2.4.0-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2014/month=9/day=1/.part-r-00008.gz.parquet.crc\n",
      "spark-2.4.0-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/\n",
      "spark-2.4.0-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=10/\n",
      "spark-2.4.0-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=10/day=26/\n",
      "spark-2.4.0-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=10/day=26/part-r-00005.gz.parquet\n",
      "spark-2.4.0-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=10/day=26/.part-r-00005.gz.parquet.crc\n",
      "spark-2.4.0-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=10/day=25/\n",
      "spark-2.4.0-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=10/day=25/.part-r-00002.gz.parquet.crc\n",
      "spark-2.4.0-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=10/day=25/part-r-00002.gz.parquet\n",
      "spark-2.4.0-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=10/day=25/part-r-00004.gz.parquet\n",
      "spark-2.4.0-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=10/day=25/.part-r-00004.gz.parquet.crc\n",
      "spark-2.4.0-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=9/\n",
      "spark-2.4.0-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=9/day=1/\n",
      "spark-2.4.0-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=9/day=1/.part-r-00007.gz.parquet.crc\n",
      "spark-2.4.0-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=9/day=1/part-r-00007.gz.parquet\n",
      "spark-2.4.0-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/_common_metadata\n",
      "spark-2.4.0-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/_metadata\n",
      "spark-2.4.0-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/_SUCCESS\n",
      "spark-2.4.0-bin-hadoop2.7/python/test_support/sql/streaming/\n",
      "spark-2.4.0-bin-hadoop2.7/python/test_support/sql/streaming/text-test.txt\n",
      "spark-2.4.0-bin-hadoop2.7/python/test_support/sql/ages.csv\n",
      "spark-2.4.0-bin-hadoop2.7/python/test_support/sql/people1.json\n",
      "spark-2.4.0-bin-hadoop2.7/python/test_support/sql/orc_partitioned/\n",
      "spark-2.4.0-bin-hadoop2.7/python/test_support/sql/orc_partitioned/b=0/\n",
      "spark-2.4.0-bin-hadoop2.7/python/test_support/sql/orc_partitioned/b=0/c=0/\n",
      "spark-2.4.0-bin-hadoop2.7/python/test_support/sql/orc_partitioned/b=0/c=0/part-r-00000-829af031-b970-49d6-ad39-30460a0be2c8.orc\n",
      "spark-2.4.0-bin-hadoop2.7/python/test_support/sql/orc_partitioned/b=0/c=0/.part-r-00000-829af031-b970-49d6-ad39-30460a0be2c8.orc.crc\n",
      "spark-2.4.0-bin-hadoop2.7/python/test_support/sql/orc_partitioned/_SUCCESS\n",
      "spark-2.4.0-bin-hadoop2.7/python/test_support/sql/orc_partitioned/b=1/\n",
      "spark-2.4.0-bin-hadoop2.7/python/test_support/sql/orc_partitioned/b=1/c=1/\n",
      "spark-2.4.0-bin-hadoop2.7/python/test_support/sql/orc_partitioned/b=1/c=1/part-r-00000-829af031-b970-49d6-ad39-30460a0be2c8.orc\n",
      "spark-2.4.0-bin-hadoop2.7/python/test_support/sql/orc_partitioned/b=1/c=1/.part-r-00000-829af031-b970-49d6-ad39-30460a0be2c8.orc.crc\n",
      "spark-2.4.0-bin-hadoop2.7/python/test_support/sql/people_array_utf16le.json\n",
      "spark-2.4.0-bin-hadoop2.7/python/run-tests.py\n",
      "spark-2.4.0-bin-hadoop2.7/python/run-tests\n",
      "spark-2.4.0-bin-hadoop2.7/python/.coveragerc\n",
      "spark-2.4.0-bin-hadoop2.7/python/lib/\n",
      "spark-2.4.0-bin-hadoop2.7/python/lib/pyspark.zip\n",
      "spark-2.4.0-bin-hadoop2.7/python/lib/PY4J_LICENSE.txt\n",
      "spark-2.4.0-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip\n",
      "spark-2.4.0-bin-hadoop2.7/python/setup.py\n",
      "spark-2.4.0-bin-hadoop2.7/python/README.md\n",
      "spark-2.4.0-bin-hadoop2.7/python/dist/\n",
      "spark-2.4.0-bin-hadoop2.7/python/pylintrc\n",
      "spark-2.4.0-bin-hadoop2.7/conf/\n",
      "spark-2.4.0-bin-hadoop2.7/conf/fairscheduler.xml.template\n",
      "spark-2.4.0-bin-hadoop2.7/conf/spark-defaults.conf.template\n",
      "spark-2.4.0-bin-hadoop2.7/conf/docker.properties.template\n",
      "spark-2.4.0-bin-hadoop2.7/conf/log4j.properties.template\n",
      "spark-2.4.0-bin-hadoop2.7/conf/metrics.properties.template\n",
      "spark-2.4.0-bin-hadoop2.7/conf/slaves.template\n",
      "spark-2.4.0-bin-hadoop2.7/conf/spark-env.sh.template\n",
      "spark-2.4.0-bin-hadoop2.7/licenses/\n",
      "spark-2.4.0-bin-hadoop2.7/licenses/LICENSE-heapq.txt\n",
      "spark-2.4.0-bin-hadoop2.7/licenses/LICENSE-paranamer.txt\n",
      "spark-2.4.0-bin-hadoop2.7/licenses/LICENSE-graphlib-dot.txt\n",
      "spark-2.4.0-bin-hadoop2.7/licenses/LICENSE-minlog.txt\n",
      "spark-2.4.0-bin-hadoop2.7/licenses/LICENSE-slf4j.txt\n",
      "spark-2.4.0-bin-hadoop2.7/licenses/LICENSE-jquery.txt\n",
      "spark-2.4.0-bin-hadoop2.7/licenses/LICENSE-scopt.txt\n",
      "spark-2.4.0-bin-hadoop2.7/licenses/LICENSE-vis.txt\n",
      "spark-2.4.0-bin-hadoop2.7/licenses/LICENSE-sbt-launch-lib.txt\n",
      "spark-2.4.0-bin-hadoop2.7/licenses/LICENSE-spire.txt\n",
      "spark-2.4.0-bin-hadoop2.7/licenses/LICENSE-matchMedia-polyfill.txt\n",
      "spark-2.4.0-bin-hadoop2.7/licenses/LICENSE-kryo.txt\n",
      "spark-2.4.0-bin-hadoop2.7/licenses/LICENSE-arpack.txt\n",
      "spark-2.4.0-bin-hadoop2.7/licenses/LICENSE-scala.txt\n",
      "spark-2.4.0-bin-hadoop2.7/licenses/LICENSE-javassist.html\n",
      "spark-2.4.0-bin-hadoop2.7/licenses/LICENSE-json-formatter.txt\n",
      "spark-2.4.0-bin-hadoop2.7/licenses/LICENSE-jodd.txt\n",
      "spark-2.4.0-bin-hadoop2.7/licenses/LICENSE-automaton.txt\n",
      "spark-2.4.0-bin-hadoop2.7/licenses/LICENSE-janino.txt\n",
      "spark-2.4.0-bin-hadoop2.7/licenses/LICENSE-dagre-d3.txt\n",
      "spark-2.4.0-bin-hadoop2.7/licenses/LICENSE-sorttable.js.txt\n",
      "spark-2.4.0-bin-hadoop2.7/licenses/LICENSE-py4j.txt\n",
      "spark-2.4.0-bin-hadoop2.7/licenses/LICENSE-d3.min.js.txt\n",
      "spark-2.4.0-bin-hadoop2.7/licenses/LICENSE-respond.txt\n",
      "spark-2.4.0-bin-hadoop2.7/licenses/LICENSE-CC0.txt\n",
      "spark-2.4.0-bin-hadoop2.7/licenses/LICENSE-zstd.txt\n",
      "spark-2.4.0-bin-hadoop2.7/licenses/LICENSE-cloudpickle.txt\n",
      "spark-2.4.0-bin-hadoop2.7/licenses/LICENSE-pmml-model.txt\n",
      "spark-2.4.0-bin-hadoop2.7/licenses/LICENSE-pyrolite.txt\n",
      "spark-2.4.0-bin-hadoop2.7/licenses/LICENSE-netlib.txt\n",
      "spark-2.4.0-bin-hadoop2.7/licenses/LICENSE-AnchorJS.txt\n",
      "spark-2.4.0-bin-hadoop2.7/licenses/LICENSE-modernizr.txt\n",
      "spark-2.4.0-bin-hadoop2.7/licenses/LICENSE-machinist.txt\n",
      "spark-2.4.0-bin-hadoop2.7/licenses/LICENSE-javolution.txt\n",
      "spark-2.4.0-bin-hadoop2.7/licenses/LICENSE-xmlenc.txt\n",
      "spark-2.4.0-bin-hadoop2.7/licenses/LICENSE-join.txt\n",
      "spark-2.4.0-bin-hadoop2.7/licenses/LICENSE-jline.txt\n",
      "spark-2.4.0-bin-hadoop2.7/licenses/LICENSE-antlr.txt\n",
      "spark-2.4.0-bin-hadoop2.7/licenses/LICENSE-jtransforms.html\n",
      "spark-2.4.0-bin-hadoop2.7/licenses/LICENSE-leveldbjni.txt\n",
      "spark-2.4.0-bin-hadoop2.7/licenses/LICENSE-protobuf.txt\n",
      "spark-2.4.0-bin-hadoop2.7/licenses/LICENSE-bootstrap.txt\n",
      "spark-2.4.0-bin-hadoop2.7/licenses/LICENSE-zstd-jni.txt\n",
      "spark-2.4.0-bin-hadoop2.7/licenses/LICENSE-datatables.txt\n",
      "spark-2.4.0-bin-hadoop2.7/licenses/LICENSE-f2j.txt\n",
      "spark-2.4.0-bin-hadoop2.7/licenses/LICENSE-reflectasm.txt\n",
      "spark-2.4.0-bin-hadoop2.7/sbin/\n",
      "spark-2.4.0-bin-hadoop2.7/sbin/start-slave.sh\n",
      "spark-2.4.0-bin-hadoop2.7/sbin/stop-history-server.sh\n",
      "spark-2.4.0-bin-hadoop2.7/sbin/stop-slave.sh\n",
      "spark-2.4.0-bin-hadoop2.7/sbin/stop-mesos-dispatcher.sh\n",
      "spark-2.4.0-bin-hadoop2.7/sbin/start-all.sh\n",
      "spark-2.4.0-bin-hadoop2.7/sbin/spark-daemon.sh\n",
      "spark-2.4.0-bin-hadoop2.7/sbin/start-master.sh\n",
      "spark-2.4.0-bin-hadoop2.7/sbin/spark-daemons.sh\n",
      "spark-2.4.0-bin-hadoop2.7/sbin/spark-config.sh\n",
      "spark-2.4.0-bin-hadoop2.7/sbin/start-slaves.sh\n",
      "spark-2.4.0-bin-hadoop2.7/sbin/start-mesos-shuffle-service.sh\n",
      "spark-2.4.0-bin-hadoop2.7/sbin/start-history-server.sh\n",
      "spark-2.4.0-bin-hadoop2.7/sbin/start-shuffle-service.sh\n",
      "spark-2.4.0-bin-hadoop2.7/sbin/slaves.sh\n",
      "spark-2.4.0-bin-hadoop2.7/sbin/stop-slaves.sh\n",
      "spark-2.4.0-bin-hadoop2.7/sbin/stop-thriftserver.sh\n",
      "spark-2.4.0-bin-hadoop2.7/sbin/stop-master.sh\n",
      "spark-2.4.0-bin-hadoop2.7/sbin/stop-shuffle-service.sh\n",
      "spark-2.4.0-bin-hadoop2.7/sbin/start-mesos-dispatcher.sh\n",
      "spark-2.4.0-bin-hadoop2.7/sbin/stop-mesos-shuffle-service.sh\n",
      "spark-2.4.0-bin-hadoop2.7/sbin/start-thriftserver.sh\n",
      "spark-2.4.0-bin-hadoop2.7/sbin/stop-all.sh\n",
      "spark-2.4.0-bin-hadoop2.7/kubernetes/\n",
      "spark-2.4.0-bin-hadoop2.7/kubernetes/tests/\n",
      "spark-2.4.0-bin-hadoop2.7/kubernetes/tests/pyfiles.py\n",
      "spark-2.4.0-bin-hadoop2.7/kubernetes/tests/worker_memory_check.py\n",
      "spark-2.4.0-bin-hadoop2.7/kubernetes/tests/py_container_checks.py\n",
      "spark-2.4.0-bin-hadoop2.7/kubernetes/dockerfiles/\n",
      "spark-2.4.0-bin-hadoop2.7/kubernetes/dockerfiles/spark/\n",
      "spark-2.4.0-bin-hadoop2.7/kubernetes/dockerfiles/spark/Dockerfile\n",
      "spark-2.4.0-bin-hadoop2.7/kubernetes/dockerfiles/spark/bindings/\n",
      "spark-2.4.0-bin-hadoop2.7/kubernetes/dockerfiles/spark/bindings/python/\n",
      "spark-2.4.0-bin-hadoop2.7/kubernetes/dockerfiles/spark/bindings/python/Dockerfile\n",
      "spark-2.4.0-bin-hadoop2.7/kubernetes/dockerfiles/spark/bindings/R/\n",
      "spark-2.4.0-bin-hadoop2.7/kubernetes/dockerfiles/spark/bindings/R/Dockerfile\n",
      "spark-2.4.0-bin-hadoop2.7/kubernetes/dockerfiles/spark/entrypoint.sh\n",
      "spark-2.4.0-bin-hadoop2.7/jars/\n",
      "spark-2.4.0-bin-hadoop2.7/jars/commons-lang3-3.5.jar\n",
      "spark-2.4.0-bin-hadoop2.7/jars/arrow-format-0.10.0.jar\n",
      "spark-2.4.0-bin-hadoop2.7/jars/hadoop-yarn-common-2.7.3.jar\n",
      "spark-2.4.0-bin-hadoop2.7/jars/hadoop-mapreduce-client-jobclient-2.7.3.jar\n",
      "spark-2.4.0-bin-hadoop2.7/jars/json4s-jackson_2.11-3.5.3.jar\n",
      "spark-2.4.0-bin-hadoop2.7/jars/jersey-container-servlet-2.22.2.jar\n",
      "spark-2.4.0-bin-hadoop2.7/jars/hive-jdbc-1.2.1.spark2.jar\n",
      "spark-2.4.0-bin-hadoop2.7/jars/curator-framework-2.7.1.jar\n",
      "spark-2.4.0-bin-hadoop2.7/jars/parquet-column-1.10.0.jar\n",
      "spark-2.4.0-bin-hadoop2.7/jars/pyrolite-4.13.jar\n",
      "spark-2.4.0-bin-hadoop2.7/jars/bonecp-0.8.0.RELEASE.jar\n",
      "spark-2.4.0-bin-hadoop2.7/jars/hppc-0.7.2.jar\n",
      "spark-2.4.0-bin-hadoop2.7/jars/metrics-jvm-3.1.5.jar\n",
      "spark-2.4.0-bin-hadoop2.7/jars/jersey-guava-2.22.2.jar\n",
      "spark-2.4.0-bin-hadoop2.7/jars/guice-3.0.jar\n",
      "spark-2.4.0-bin-hadoop2.7/jars/super-csv-2.2.0.jar\n",
      "spark-2.4.0-bin-hadoop2.7/jars/jsp-api-2.1.jar\n",
      "spark-2.4.0-bin-hadoop2.7/jars/commons-net-3.1.jar\n",
      "spark-2.4.0-bin-hadoop2.7/jars/validation-api-1.1.0.Final.jar\n",
      "spark-2.4.0-bin-hadoop2.7/jars/json4s-ast_2.11-3.5.3.jar\n",
      "spark-2.4.0-bin-hadoop2.7/jars/javax.inject-1.jar\n",
      "spark-2.4.0-bin-hadoop2.7/jars/hadoop-mapreduce-client-shuffle-2.7.3.jar\n",
      "spark-2.4.0-bin-hadoop2.7/jars/avro-ipc-1.8.2.jar\n",
      "spark-2.4.0-bin-hadoop2.7/jars/spark-network-shuffle_2.11-2.4.0.jar\n",
      "spark-2.4.0-bin-hadoop2.7/jars/orc-shims-1.5.2.jar\n",
      "spark-2.4.0-bin-hadoop2.7/jars/jersey-container-servlet-core-2.22.2.jar\n",
      "spark-2.4.0-bin-hadoop2.7/jars/zjsonpatch-0.3.0.jar\n",
      "spark-2.4.0-bin-hadoop2.7/jars/scala-compiler-2.11.12.jar\n",
      "spark-2.4.0-bin-hadoop2.7/jars/spark-graphx_2.11-2.4.0.jar\n",
      "spark-2.4.0-bin-hadoop2.7/jars/curator-client-2.7.1.jar\n",
      "spark-2.4.0-bin-hadoop2.7/jars/jersey-media-jaxb-2.22.2.jar\n",
      "spark-2.4.0-bin-hadoop2.7/jars/avro-mapred-1.8.2-hadoop2.jar\n",
      "spark-2.4.0-bin-hadoop2.7/jars/antlr4-runtime-4.7.jar\n",
      "spark-2.4.0-bin-hadoop2.7/jars/spark-kubernetes_2.11-2.4.0.jar\n",
      "spark-2.4.0-bin-hadoop2.7/jars/jackson-module-jaxb-annotations-2.6.7.jar\n",
      "spark-2.4.0-bin-hadoop2.7/jars/okhttp-3.8.1.jar\n",
      "spark-2.4.0-bin-hadoop2.7/jars/leveldbjni-all-1.8.jar\n",
      "spark-2.4.0-bin-hadoop2.7/jars/spark-tags_2.11-2.4.0-tests.jar\n",
      "spark-2.4.0-bin-hadoop2.7/jars/jackson-mapper-asl-1.9.13.jar\n",
      "spark-2.4.0-bin-hadoop2.7/jars/ST4-4.0.4.jar\n",
      "spark-2.4.0-bin-hadoop2.7/jars/eigenbase-properties-1.1.5.jar\n",
      "spark-2.4.0-bin-hadoop2.7/jars/jackson-jaxrs-1.9.13.jar\n",
      "spark-2.4.0-bin-hadoop2.7/jars/commons-beanutils-1.7.0.jar\n",
      "spark-2.4.0-bin-hadoop2.7/jars/jackson-annotations-2.6.7.jar\n",
      "spark-2.4.0-bin-hadoop2.7/jars/kubernetes-model-2.0.0.jar\n",
      "spark-2.4.0-bin-hadoop2.7/jars/spark-tags_2.11-2.4.0.jar\n",
      "spark-2.4.0-bin-hadoop2.7/jars/jackson-module-paranamer-2.7.9.jar\n",
      "spark-2.4.0-bin-hadoop2.7/jars/jdo-api-3.0.1.jar\n",
      "spark-2.4.0-bin-hadoop2.7/jars/jersey-server-2.22.2.jar\n",
      "spark-2.4.0-bin-hadoop2.7/jars/scala-parser-combinators_2.11-1.1.0.jar\n",
      "spark-2.4.0-bin-hadoop2.7/jars/snappy-java-1.1.7.1.jar\n",
      "spark-2.4.0-bin-hadoop2.7/jars/arrow-memory-0.10.0.jar\n",
      "spark-2.4.0-bin-hadoop2.7/jars/hk2-utils-2.4.0-b34.jar\n",
      "spark-2.4.0-bin-hadoop2.7/jars/spark-repl_2.11-2.4.0.jar\n",
      "spark-2.4.0-bin-hadoop2.7/jars/joda-time-2.9.3.jar\n",
      "spark-2.4.0-bin-hadoop2.7/jars/commons-crypto-1.0.0.jar\n",
      "spark-2.4.0-bin-hadoop2.7/jars/aopalliance-1.0.jar\n",
      "spark-2.4.0-bin-hadoop2.7/jars/log4j-1.2.17.jar\n",
      "spark-2.4.0-bin-hadoop2.7/jars/commons-pool-1.5.4.jar\n",
      "spark-2.4.0-bin-hadoop2.7/jars/xz-1.5.jar\n",
      "spark-2.4.0-bin-hadoop2.7/jars/hk2-locator-2.4.0-b34.jar\n",
      "spark-2.4.0-bin-hadoop2.7/jars/libthrift-0.9.3.jar\n",
      "spark-2.4.0-bin-hadoop2.7/jars/chill-java-0.9.3.jar\n",
      "spark-2.4.0-bin-hadoop2.7/jars/logging-interceptor-3.8.1.jar\n",
      "spark-2.4.0-bin-hadoop2.7/jars/opencsv-2.3.jar\n",
      "spark-2.4.0-bin-hadoop2.7/jars/lz4-java-1.4.0.jar\n",
      "spark-2.4.0-bin-hadoop2.7/jars/spark-streaming_2.11-2.4.0.jar\n",
      "spark-2.4.0-bin-hadoop2.7/jars/slf4j-log4j12-1.7.16.jar\n",
      "spark-2.4.0-bin-hadoop2.7/jars/calcite-core-1.2.0-incubating.jar\n",
      "spark-2.4.0-bin-hadoop2.7/jars/breeze_2.11-0.13.2.jar\n",
      "spark-2.4.0-bin-hadoop2.7/jars/parquet-hadoop-bundle-1.6.0.jar\n",
      "spark-2.4.0-bin-hadoop2.7/jars/spark-hive-thriftserver_2.11-2.4.0.jar\n",
      "spark-2.4.0-bin-hadoop2.7/jars/javax.inject-2.4.0-b34.jar\n",
      "spark-2.4.0-bin-hadoop2.7/jars/metrics-json-3.1.5.jar\n",
      "spark-2.4.0-bin-hadoop2.7/jars/aopalliance-repackaged-2.4.0-b34.jar\n",
      "spark-2.4.0-bin-hadoop2.7/jars/spark-mesos_2.11-2.4.0.jar\n",
      "spark-2.4.0-bin-hadoop2.7/jars/jcl-over-slf4j-1.7.16.jar\n",
      "spark-2.4.0-bin-hadoop2.7/jars/antlr-runtime-3.4.jar\n",
      "spark-2.4.0-bin-hadoop2.7/jars/httpcore-4.4.10.jar\n",
      "spark-2.4.0-bin-hadoop2.7/jars/metrics-graphite-3.1.5.jar\n",
      "spark-2.4.0-bin-hadoop2.7/jars/netty-3.9.9.Final.jar\n",
      "spark-2.4.0-bin-hadoop2.7/jars/guava-14.0.1.jar\n",
      "spark-2.4.0-bin-hadoop2.7/jars/jaxb-api-2.2.2.jar\n",
      "spark-2.4.0-bin-hadoop2.7/jars/datanucleus-core-3.2.10.jar\n",
      "spark-2.4.0-bin-hadoop2.7/jars/compress-lzf-1.0.3.jar\n",
      "spark-2.4.0-bin-hadoop2.7/jars/antlr-2.7.7.jar\n",
      "spark-2.4.0-bin-hadoop2.7/jars/jersey-common-2.22.2.jar\n",
      "spark-2.4.0-bin-hadoop2.7/jars/hadoop-yarn-client-2.7.3.jar\n",
      "spark-2.4.0-bin-hadoop2.7/jars/commons-lang-2.6.jar\n",
      "spark-2.4.0-bin-hadoop2.7/jars/datanucleus-api-jdo-3.2.6.jar\n",
      "spark-2.4.0-bin-hadoop2.7/jars/commons-digester-1.8.jar\n",
      "spark-2.4.0-bin-hadoop2.7/jars/hadoop-annotations-2.7.3.jar\n",
      "spark-2.4.0-bin-hadoop2.7/jars/jtransforms-2.4.0.jar\n",
      "spark-2.4.0-bin-hadoop2.7/jars/jodd-core-3.5.2.jar\n",
      "spark-2.4.0-bin-hadoop2.7/jars/snakeyaml-1.15.jar\n",
      "spark-2.4.0-bin-hadoop2.7/jars/commons-cli-1.2.jar\n",
      "spark-2.4.0-bin-hadoop2.7/jars/parquet-format-2.4.0.jar\n",
      "spark-2.4.0-bin-hadoop2.7/jars/commons-configuration-1.6.jar\n",
      "spark-2.4.0-bin-hadoop2.7/jars/jpam-1.1.jar\n",
      "spark-2.4.0-bin-hadoop2.7/jars/kryo-shaded-4.0.2.jar\n",
      "spark-2.4.0-bin-hadoop2.7/jars/zookeeper-3.4.6.jar\n",
      "spark-2.4.0-bin-hadoop2.7/jars/javax.ws.rs-api-2.0.1.jar\n",
      "spark-2.4.0-bin-hadoop2.7/jars/jackson-core-asl-1.9.13.jar\n",
      "spark-2.4.0-bin-hadoop2.7/jars/hadoop-mapreduce-client-core-2.7.3.jar\n",
      "spark-2.4.0-bin-hadoop2.7/jars/json4s-scalap_2.11-3.5.3.jar\n",
      "spark-2.4.0-bin-hadoop2.7/jars/spark-kvstore_2.11-2.4.0.jar\n",
      "spark-2.4.0-bin-hadoop2.7/jars/avro-1.8.2.jar\n",
      "spark-2.4.0-bin-hadoop2.7/jars/hadoop-yarn-api-2.7.3.jar\n",
      "spark-2.4.0-bin-hadoop2.7/jars/api-asn1-api-1.0.0-M20.jar\n",
      "spark-2.4.0-bin-hadoop2.7/jars/paranamer-2.8.jar\n",
      "spark-2.4.0-bin-hadoop2.7/jars/RoaringBitmap-0.5.11.jar\n",
      "spark-2.4.0-bin-hadoop2.7/jars/janino-3.0.9.jar\n",
      "spark-2.4.0-bin-hadoop2.7/jars/stringtemplate-3.2.1.jar\n",
      "spark-2.4.0-bin-hadoop2.7/jars/apacheds-i18n-2.0.0-M15.jar\n",
      "spark-2.4.0-bin-hadoop2.7/jars/apache-log4j-extras-1.2.17.jar\n",
      "spark-2.4.0-bin-hadoop2.7/jars/xbean-asm6-shaded-4.8.jar\n",
      "spark-2.4.0-bin-hadoop2.7/jars/curator-recipes-2.7.1.jar\n",
      "spark-2.4.0-bin-hadoop2.7/jars/oro-2.0.8.jar\n",
      "spark-2.4.0-bin-hadoop2.7/jars/javax.annotation-api-1.2.jar\n",
      "spark-2.4.0-bin-hadoop2.7/jars/chill_2.11-0.9.3.jar\n",
      "spark-2.4.0-bin-hadoop2.7/jars/spark-mllib_2.11-2.4.0.jar\n",
      "spark-2.4.0-bin-hadoop2.7/jars/hive-exec-1.2.1.spark2.jar\n",
      "spark-2.4.0-bin-hadoop2.7/jars/hadoop-common-2.7.3.jar\n",
      "spark-2.4.0-bin-hadoop2.7/jars/commons-dbcp-1.4.jar\n",
      "spark-2.4.0-bin-hadoop2.7/jars/mesos-1.4.0-shaded-protobuf.jar\n",
      "spark-2.4.0-bin-hadoop2.7/jars/spark-core_2.11-2.4.0.jar\n",
      "spark-2.4.0-bin-hadoop2.7/jars/okio-1.13.0.jar\n",
      "spark-2.4.0-bin-hadoop2.7/jars/jackson-databind-2.6.7.1.jar\n",
      "spark-2.4.0-bin-hadoop2.7/jars/spark-catalyst_2.11-2.4.0.jar\n",
      "spark-2.4.0-bin-hadoop2.7/jars/netty-all-4.1.17.Final.jar\n",
      "spark-2.4.0-bin-hadoop2.7/jars/commons-compiler-3.0.9.jar\n",
      "spark-2.4.0-bin-hadoop2.7/jars/parquet-hadoop-1.10.0.jar\n",
      "spark-2.4.0-bin-hadoop2.7/jars/commons-math3-3.4.1.jar\n",
      "spark-2.4.0-bin-hadoop2.7/jars/httpclient-4.5.6.jar\n",
      "spark-2.4.0-bin-hadoop2.7/jars/jul-to-slf4j-1.7.16.jar\n",
      "spark-2.4.0-bin-hadoop2.7/jars/stax-api-1.0-2.jar\n",
      "spark-2.4.0-bin-hadoop2.7/jars/core-1.1.2.jar\n",
      "spark-2.4.0-bin-hadoop2.7/jars/commons-codec-1.10.jar\n",
      "spark-2.4.0-bin-hadoop2.7/jars/gson-2.2.4.jar\n",
      "spark-2.4.0-bin-hadoop2.7/jars/zstd-jni-1.3.2-2.jar\n",
      "spark-2.4.0-bin-hadoop2.7/jars/javolution-5.5.1.jar\n",
      "spark-2.4.0-bin-hadoop2.7/jars/univocity-parsers-2.7.3.jar\n",
      "spark-2.4.0-bin-hadoop2.7/jars/apacheds-kerberos-codec-2.0.0-M15.jar\n",
      "spark-2.4.0-bin-hadoop2.7/jars/kubernetes-client-3.0.0.jar\n",
      "spark-2.4.0-bin-hadoop2.7/jars/parquet-common-1.10.0.jar\n",
      "spark-2.4.0-bin-hadoop2.7/jars/hadoop-yarn-server-common-2.7.3.jar\n",
      "spark-2.4.0-bin-hadoop2.7/jars/libfb303-0.9.3.jar\n",
      "spark-2.4.0-bin-hadoop2.7/jars/orc-mapreduce-1.5.2-nohive.jar\n",
      "spark-2.4.0-bin-hadoop2.7/jars/jersey-client-2.22.2.jar\n",
      "spark-2.4.0-bin-hadoop2.7/jars/xercesImpl-2.9.1.jar\n",
      "spark-2.4.0-bin-hadoop2.7/jars/commons-collections-3.2.2.jar\n",
      "spark-2.4.0-bin-hadoop2.7/jars/machinist_2.11-0.6.1.jar\n",
      "spark-2.4.0-bin-hadoop2.7/jars/htrace-core-3.1.0-incubating.jar\n",
      "spark-2.4.0-bin-hadoop2.7/jars/jta-1.1.jar\n",
      "spark-2.4.0-bin-hadoop2.7/jars/osgi-resource-locator-1.0.1.jar\n",
      "spark-2.4.0-bin-hadoop2.7/jars/datanucleus-rdbms-3.2.9.jar\n",
      "spark-2.4.0-bin-hadoop2.7/jars/spark-unsafe_2.11-2.4.0.jar\n",
      "spark-2.4.0-bin-hadoop2.7/jars/javassist-3.18.1-GA.jar\n",
      "spark-2.4.0-bin-hadoop2.7/jars/spire-macros_2.11-0.13.0.jar\n",
      "spark-2.4.0-bin-hadoop2.7/jars/calcite-linq4j-1.2.0-incubating.jar\n",
      "spark-2.4.0-bin-hadoop2.7/jars/jline-2.14.6.jar\n",
      "spark-2.4.0-bin-hadoop2.7/jars/spark-launcher_2.11-2.4.0.jar\n",
      "spark-2.4.0-bin-hadoop2.7/jars/hadoop-client-2.7.3.jar\n",
      "spark-2.4.0-bin-hadoop2.7/jars/hive-metastore-1.2.1.spark2.jar\n",
      "spark-2.4.0-bin-hadoop2.7/jars/hadoop-yarn-server-web-proxy-2.7.3.jar\n",
      "spark-2.4.0-bin-hadoop2.7/jars/spire_2.11-0.13.0.jar\n",
      "spark-2.4.0-bin-hadoop2.7/jars/jackson-core-2.6.7.jar\n",
      "spark-2.4.0-bin-hadoop2.7/jars/api-util-1.0.0-M20.jar\n",
      "spark-2.4.0-bin-hadoop2.7/jars/commons-httpclient-3.1.jar\n",
      "spark-2.4.0-bin-hadoop2.7/jars/orc-core-1.5.2-nohive.jar\n",
      "spark-2.4.0-bin-hadoop2.7/jars/aircompressor-0.10.jar\n",
      "spark-2.4.0-bin-hadoop2.7/jars/guice-servlet-3.0.jar\n",
      "spark-2.4.0-bin-hadoop2.7/jars/scala-xml_2.11-1.0.5.jar\n",
      "spark-2.4.0-bin-hadoop2.7/jars/hadoop-mapreduce-client-common-2.7.3.jar\n",
      "spark-2.4.0-bin-hadoop2.7/jars/spark-hive_2.11-2.4.0.jar\n",
      "spark-2.4.0-bin-hadoop2.7/jars/protobuf-java-2.5.0.jar\n",
      "spark-2.4.0-bin-hadoop2.7/jars/parquet-jackson-1.10.0.jar\n",
      "spark-2.4.0-bin-hadoop2.7/jars/spark-sketch_2.11-2.4.0.jar\n",
      "spark-2.4.0-bin-hadoop2.7/jars/hive-beeline-1.2.1.spark2.jar\n",
      "spark-2.4.0-bin-hadoop2.7/jars/spark-network-common_2.11-2.4.0.jar\n",
      "spark-2.4.0-bin-hadoop2.7/jars/commons-beanutils-core-1.8.0.jar\n",
      "spark-2.4.0-bin-hadoop2.7/jars/stream-2.7.0.jar\n",
      "spark-2.4.0-bin-hadoop2.7/jars/commons-compress-1.8.1.jar\n",
      "spark-2.4.0-bin-hadoop2.7/jars/macro-compat_2.11-1.1.1.jar\n",
      "spark-2.4.0-bin-hadoop2.7/jars/activation-1.1.1.jar\n",
      "spark-2.4.0-bin-hadoop2.7/jars/breeze-macros_2.11-0.13.2.jar\n",
      "spark-2.4.0-bin-hadoop2.7/jars/derby-10.12.1.1.jar\n",
      "spark-2.4.0-bin-hadoop2.7/jars/shapeless_2.11-2.3.2.jar\n",
      "spark-2.4.0-bin-hadoop2.7/jars/minlog-1.3.0.jar\n",
      "spark-2.4.0-bin-hadoop2.7/jars/commons-logging-1.1.3.jar\n",
      "spark-2.4.0-bin-hadoop2.7/jars/spark-sql_2.11-2.4.0.jar\n",
      "spark-2.4.0-bin-hadoop2.7/jars/spark-mllib-local_2.11-2.4.0.jar\n",
      "spark-2.4.0-bin-hadoop2.7/jars/hive-cli-1.2.1.spark2.jar\n",
      "spark-2.4.0-bin-hadoop2.7/jars/metrics-core-3.1.5.jar\n",
      "spark-2.4.0-bin-hadoop2.7/jars/jackson-module-scala_2.11-2.6.7.1.jar\n",
      "spark-2.4.0-bin-hadoop2.7/jars/automaton-1.11-8.jar\n",
      "spark-2.4.0-bin-hadoop2.7/jars/objenesis-2.5.1.jar\n",
      "spark-2.4.0-bin-hadoop2.7/jars/json4s-core_2.11-3.5.3.jar\n",
      "spark-2.4.0-bin-hadoop2.7/jars/py4j-0.10.7.jar\n",
      "spark-2.4.0-bin-hadoop2.7/jars/jetty-6.1.26.jar\n",
      "spark-2.4.0-bin-hadoop2.7/jars/slf4j-api-1.7.16.jar\n",
      "spark-2.4.0-bin-hadoop2.7/jars/commons-io-2.4.jar\n",
      "spark-2.4.0-bin-hadoop2.7/jars/jackson-xc-1.9.13.jar\n",
      "spark-2.4.0-bin-hadoop2.7/jars/scala-reflect-2.11.12.jar\n",
      "spark-2.4.0-bin-hadoop2.7/jars/xmlenc-0.52.jar\n",
      "spark-2.4.0-bin-hadoop2.7/jars/spark-yarn_2.11-2.4.0.jar\n",
      "spark-2.4.0-bin-hadoop2.7/jars/jetty-util-6.1.26.jar\n",
      "spark-2.4.0-bin-hadoop2.7/jars/ivy-2.4.0.jar\n",
      "spark-2.4.0-bin-hadoop2.7/jars/scala-library-2.11.12.jar\n",
      "spark-2.4.0-bin-hadoop2.7/jars/snappy-0.2.jar\n",
      "spark-2.4.0-bin-hadoop2.7/jars/jsr305-1.3.9.jar\n",
      "spark-2.4.0-bin-hadoop2.7/jars/arpack_combined_all-0.1.jar\n",
      "spark-2.4.0-bin-hadoop2.7/jars/stax-api-1.0.1.jar\n",
      "spark-2.4.0-bin-hadoop2.7/jars/hadoop-mapreduce-client-app-2.7.3.jar\n",
      "spark-2.4.0-bin-hadoop2.7/jars/hk2-api-2.4.0-b34.jar\n",
      "spark-2.4.0-bin-hadoop2.7/jars/jackson-dataformat-yaml-2.6.7.jar\n",
      "spark-2.4.0-bin-hadoop2.7/jars/arrow-vector-0.10.0.jar\n",
      "spark-2.4.0-bin-hadoop2.7/jars/flatbuffers-1.2.0-3f79e055.jar\n",
      "spark-2.4.0-bin-hadoop2.7/jars/calcite-avatica-1.2.0-incubating.jar\n",
      "spark-2.4.0-bin-hadoop2.7/jars/generex-1.0.1.jar\n",
      "spark-2.4.0-bin-hadoop2.7/jars/JavaEWAH-0.3.2.jar\n",
      "spark-2.4.0-bin-hadoop2.7/jars/parquet-encoding-1.10.0.jar\n",
      "spark-2.4.0-bin-hadoop2.7/jars/javax.servlet-api-3.1.0.jar\n",
      "spark-2.4.0-bin-hadoop2.7/jars/hadoop-hdfs-2.7.3.jar\n",
      "spark-2.4.0-bin-hadoop2.7/jars/hadoop-auth-2.7.3.jar\n",
      "spark-2.4.0-bin-hadoop2.7/bin/\n",
      "spark-2.4.0-bin-hadoop2.7/bin/beeline\n",
      "spark-2.4.0-bin-hadoop2.7/bin/pyspark\n",
      "spark-2.4.0-bin-hadoop2.7/bin/pyspark.cmd\n",
      "spark-2.4.0-bin-hadoop2.7/bin/find-spark-home\n",
      "spark-2.4.0-bin-hadoop2.7/bin/spark-submit2.cmd\n",
      "spark-2.4.0-bin-hadoop2.7/bin/spark-class.cmd\n",
      "spark-2.4.0-bin-hadoop2.7/bin/run-example\n",
      "spark-2.4.0-bin-hadoop2.7/bin/sparkR2.cmd\n",
      "spark-2.4.0-bin-hadoop2.7/bin/spark-sql2.cmd\n",
      "spark-2.4.0-bin-hadoop2.7/bin/spark-submit.cmd\n",
      "spark-2.4.0-bin-hadoop2.7/bin/run-example.cmd\n",
      "spark-2.4.0-bin-hadoop2.7/bin/sparkR.cmd\n",
      "spark-2.4.0-bin-hadoop2.7/bin/spark-class2.cmd\n",
      "spark-2.4.0-bin-hadoop2.7/bin/spark-submit\n",
      "spark-2.4.0-bin-hadoop2.7/bin/sparkR\n",
      "spark-2.4.0-bin-hadoop2.7/bin/spark-sql.cmd\n",
      "spark-2.4.0-bin-hadoop2.7/bin/load-spark-env.cmd\n",
      "spark-2.4.0-bin-hadoop2.7/bin/pyspark2.cmd\n",
      "spark-2.4.0-bin-hadoop2.7/bin/docker-image-tool.sh\n",
      "spark-2.4.0-bin-hadoop2.7/bin/spark-shell\n",
      "spark-2.4.0-bin-hadoop2.7/bin/find-spark-home.cmd\n",
      "spark-2.4.0-bin-hadoop2.7/bin/spark-shell.cmd\n",
      "spark-2.4.0-bin-hadoop2.7/bin/load-spark-env.sh\n",
      "spark-2.4.0-bin-hadoop2.7/bin/spark-sql\n",
      "spark-2.4.0-bin-hadoop2.7/bin/spark-shell2.cmd\n",
      "spark-2.4.0-bin-hadoop2.7/bin/spark-class\n",
      "spark-2.4.0-bin-hadoop2.7/bin/beeline.cmd\n",
      "spark-2.4.0-bin-hadoop2.7/RELEASE\n",
      "spark-2.4.0-bin-hadoop2.7/R/\n",
      "spark-2.4.0-bin-hadoop2.7/R/lib/\n",
      "spark-2.4.0-bin-hadoop2.7/R/lib/SparkR/\n",
      "spark-2.4.0-bin-hadoop2.7/R/lib/SparkR/NAMESPACE\n",
      "spark-2.4.0-bin-hadoop2.7/R/lib/SparkR/doc/\n",
      "spark-2.4.0-bin-hadoop2.7/R/lib/SparkR/doc/sparkr-vignettes.Rmd\n",
      "spark-2.4.0-bin-hadoop2.7/R/lib/SparkR/doc/sparkr-vignettes.html\n",
      "spark-2.4.0-bin-hadoop2.7/R/lib/SparkR/doc/sparkr-vignettes.R\n",
      "spark-2.4.0-bin-hadoop2.7/R/lib/SparkR/doc/index.html\n",
      "spark-2.4.0-bin-hadoop2.7/R/lib/SparkR/DESCRIPTION\n",
      "spark-2.4.0-bin-hadoop2.7/R/lib/SparkR/R/\n",
      "spark-2.4.0-bin-hadoop2.7/R/lib/SparkR/R/SparkR.rdb\n",
      "spark-2.4.0-bin-hadoop2.7/R/lib/SparkR/R/SparkR\n",
      "spark-2.4.0-bin-hadoop2.7/R/lib/SparkR/R/SparkR.rdx\n",
      "spark-2.4.0-bin-hadoop2.7/R/lib/SparkR/html/\n",
      "spark-2.4.0-bin-hadoop2.7/R/lib/SparkR/html/R.css\n",
      "spark-2.4.0-bin-hadoop2.7/R/lib/SparkR/html/00Index.html\n",
      "spark-2.4.0-bin-hadoop2.7/R/lib/SparkR/help/\n",
      "spark-2.4.0-bin-hadoop2.7/R/lib/SparkR/help/AnIndex\n",
      "spark-2.4.0-bin-hadoop2.7/R/lib/SparkR/help/SparkR.rdb\n",
      "spark-2.4.0-bin-hadoop2.7/R/lib/SparkR/help/aliases.rds\n",
      "spark-2.4.0-bin-hadoop2.7/R/lib/SparkR/help/SparkR.rdx\n",
      "spark-2.4.0-bin-hadoop2.7/R/lib/SparkR/help/paths.rds\n",
      "spark-2.4.0-bin-hadoop2.7/R/lib/SparkR/profile/\n",
      "spark-2.4.0-bin-hadoop2.7/R/lib/SparkR/profile/general.R\n",
      "spark-2.4.0-bin-hadoop2.7/R/lib/SparkR/profile/shell.R\n",
      "spark-2.4.0-bin-hadoop2.7/R/lib/SparkR/tests/\n",
      "spark-2.4.0-bin-hadoop2.7/R/lib/SparkR/tests/testthat/\n",
      "spark-2.4.0-bin-hadoop2.7/R/lib/SparkR/tests/testthat/test_basic.R\n",
      "spark-2.4.0-bin-hadoop2.7/R/lib/SparkR/Meta/\n",
      "spark-2.4.0-bin-hadoop2.7/R/lib/SparkR/Meta/vignette.rds\n",
      "spark-2.4.0-bin-hadoop2.7/R/lib/SparkR/Meta/package.rds\n",
      "spark-2.4.0-bin-hadoop2.7/R/lib/SparkR/Meta/Rd.rds\n",
      "spark-2.4.0-bin-hadoop2.7/R/lib/SparkR/Meta/nsInfo.rds\n",
      "spark-2.4.0-bin-hadoop2.7/R/lib/SparkR/Meta/hsearch.rds\n",
      "spark-2.4.0-bin-hadoop2.7/R/lib/SparkR/Meta/features.rds\n",
      "spark-2.4.0-bin-hadoop2.7/R/lib/SparkR/Meta/links.rds\n",
      "spark-2.4.0-bin-hadoop2.7/R/lib/SparkR/worker/\n",
      "spark-2.4.0-bin-hadoop2.7/R/lib/SparkR/worker/daemon.R\n",
      "spark-2.4.0-bin-hadoop2.7/R/lib/SparkR/worker/worker.R\n",
      "spark-2.4.0-bin-hadoop2.7/R/lib/SparkR/INDEX\n",
      "spark-2.4.0-bin-hadoop2.7/R/lib/sparkr.zip\n",
      "spark-2.4.0-bin-hadoop2.7/yarn/\n",
      "spark-2.4.0-bin-hadoop2.7/yarn/spark-2.4.0-yarn-shuffle.jar\n",
      "spark-2.4.0-bin-hadoop2.7/examples/\n",
      "spark-2.4.0-bin-hadoop2.7/examples/jars/\n",
      "spark-2.4.0-bin-hadoop2.7/examples/jars/spark-examples_2.11-2.4.0.jar\n",
      "spark-2.4.0-bin-hadoop2.7/examples/jars/scopt_2.11-3.7.0.jar\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/python/\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/python/als.py\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/python/pagerank.py\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/python/mllib/\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/python/mllib/ranking_metrics_example.py\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/python/mllib/gradient_boosting_regression_example.py\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/python/mllib/decision_tree_classification_example.py\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/python/mllib/tf_idf_example.py\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/python/mllib/decision_tree_regression_example.py\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/python/mllib/k_means_example.py\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/python/mllib/streaming_linear_regression_example.py\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/python/mllib/random_forest_regression_example.py\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/python/mllib/random_forest_classification_example.py\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/python/mllib/pca_rowmatrix_example.py\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/python/mllib/elementwise_product_example.py\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/python/mllib/stratified_sampling_example.py\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/python/mllib/multi_class_metrics_example.py\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/python/mllib/gradient_boosting_classification_example.py\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/python/mllib/fpgrowth_example.py\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/python/mllib/binary_classification_metrics_example.py\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/python/mllib/sampled_rdds.py\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/python/mllib/logistic_regression_with_lbfgs_example.py\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/python/mllib/kernel_density_estimation_example.py\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/python/mllib/logistic_regression.py\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/python/mllib/hypothesis_testing_example.py\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/python/mllib/svm_with_sgd_example.py\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/python/mllib/streaming_k_means_example.py\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/python/mllib/correlations.py\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/python/mllib/correlations_example.py\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/python/mllib/regression_metrics_example.py\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/python/mllib/naive_bayes_example.py\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/python/mllib/gaussian_mixture_model.py\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/python/mllib/hypothesis_testing_kolmogorov_smirnov_test_example.py\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/python/mllib/word2vec.py\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/python/mllib/latent_dirichlet_allocation_example.py\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/python/mllib/recommendation_example.py\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/python/mllib/linear_regression_with_sgd_example.py\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/python/mllib/random_rdd_generation.py\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/python/mllib/bisecting_k_means_example.py\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/python/mllib/summary_statistics_example.py\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/python/mllib/word2vec_example.py\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/python/mllib/kmeans.py\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/python/mllib/standard_scaler_example.py\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/python/mllib/multi_label_metrics_example.py\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/python/mllib/svd_example.py\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/python/mllib/normalizer_example.py\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/python/mllib/power_iteration_clustering_example.py\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/python/mllib/isotonic_regression_example.py\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/python/mllib/gaussian_mixture_example.py\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/python/parquet_inputformat.py\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/python/logistic_regression.py\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/python/ml/\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/python/ml/rformula_example.py\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/python/ml/min_max_scaler_example.py\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/python/ml/min_hash_lsh_example.py\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/python/ml/gradient_boosted_tree_regressor_example.py\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/python/ml/decision_tree_classification_example.py\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/python/ml/logistic_regression_with_elastic_net.py\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/python/ml/tf_idf_example.py\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/python/ml/decision_tree_regression_example.py\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/python/ml/vector_slicer_example.py\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/python/ml/als_example.py\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/python/ml/cross_validator.py\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/python/ml/onehot_encoder_estimator_example.py\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/python/ml/gradient_boosted_tree_classifier_example.py\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/python/ml/generalized_linear_regression_example.py\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/python/ml/random_forest_regressor_example.py\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/python/ml/chisq_selector_example.py\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/python/ml/max_abs_scaler_example.py\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/python/ml/n_gram_example.py\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/python/ml/elementwise_product_example.py\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/python/ml/binarizer_example.py\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/python/ml/bucketed_random_projection_lsh_example.py\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/python/ml/imputer_example.py\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/python/ml/correlation_example.py\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/python/ml/count_vectorizer_example.py\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/python/ml/multilayer_perceptron_classification.py\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/python/ml/stopwords_remover_example.py\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/python/ml/fpgrowth_example.py\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/python/ml/kmeans_example.py\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/python/ml/chi_square_test_example.py\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/python/ml/aft_survival_regression.py\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/python/ml/dct_example.py\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/python/ml/dataframe_example.py\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/python/ml/pca_example.py\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/python/ml/pipeline_example.py\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/python/ml/polynomial_expansion_example.py\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/python/ml/tokenizer_example.py\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/python/ml/estimator_transformer_param_example.py\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/python/ml/naive_bayes_example.py\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/python/ml/string_indexer_example.py\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/python/ml/summarizer_example.py\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/python/ml/bucketizer_example.py\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/python/ml/vector_assembler_example.py\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/python/ml/sql_transformer.py\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/python/ml/feature_hasher_example.py\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/python/ml/vector_indexer_example.py\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/python/ml/train_validation_split.py\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/python/ml/vector_size_hint_example.py\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/python/ml/one_vs_rest_example.py\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/python/ml/logistic_regression_summary_example.py\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/python/ml/bisecting_k_means_example.py\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/python/ml/multiclass_logistic_regression_with_elastic_net.py\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/python/ml/index_to_string_example.py\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/python/ml/linear_regression_with_elastic_net.py\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/python/ml/linearsvc.py\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/python/ml/word2vec_example.py\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/python/ml/standard_scaler_example.py\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/python/ml/prefixspan_example.py\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/python/ml/lda_example.py\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/python/ml/normalizer_example.py\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/python/ml/quantile_discretizer_example.py\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/python/ml/random_forest_classifier_example.py\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/python/ml/isotonic_regression_example.py\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/python/ml/gaussian_mixture_example.py\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/python/transitive_closure.py\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/python/streaming/\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/python/streaming/flume_wordcount.py\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/python/streaming/queue_stream.py\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/python/streaming/recoverable_network_wordcount.py\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/python/streaming/kafka_wordcount.py\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/python/streaming/hdfs_wordcount.py\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/python/streaming/direct_kafka_wordcount.py\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/python/streaming/stateful_network_wordcount.py\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/python/streaming/network_wordcount.py\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/python/streaming/sql_network_wordcount.py\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/python/streaming/network_wordjoinsentiments.py\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/python/wordcount.py\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/python/status_api_demo.py\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/python/sql/\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/python/sql/basic.py\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/python/sql/arrow.py\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/python/sql/streaming/\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/python/sql/streaming/structured_kafka_wordcount.py\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/python/sql/streaming/structured_network_wordcount_windowed.py\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/python/sql/streaming/structured_network_wordcount.py\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/python/sql/datasource.py\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/python/sql/hive.py\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/python/kmeans.py\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/python/sort.py\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/python/pi.py\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/python/avro_inputformat.py\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/scala/\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/scala/org/\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/scala/org/apache/\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/LocalPi.scala\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/SparkTC.scala\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/pythonconverters/\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/pythonconverters/AvroConverters.scala\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/HdfsTest.scala\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/SparkHdfsLR.scala\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/graphx/\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/graphx/LiveJournalPageRank.scala\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/graphx/Analytics.scala\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/graphx/ConnectedComponentsExample.scala\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/graphx/TriangleCountingExample.scala\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/graphx/SynthBenchmark.scala\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/graphx/ComprehensiveExample.scala\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/graphx/SSSPExample.scala\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/graphx/AggregateMessagesExample.scala\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/graphx/PageRankExample.scala\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/SparkPageRank.scala\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/LocalKMeans.scala\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/BroadcastTest.scala\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/SparkLR.scala\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/StreamingTestExample.scala\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/StreamingLinearRegressionExample.scala\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/LatentDirichletAllocationExample.scala\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/StandardScalerExample.scala\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/GaussianMixtureExample.scala\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/ElementwiseProductExample.scala\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/LBFGSExample.scala\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/SampledRDDs.scala\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/GradientBoostedTreesRunner.scala\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/TallSkinnyPCA.scala\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/DecisionTreeRunner.scala\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/GradientBoostingClassificationExample.scala\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/PrefixSpanExample.scala\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/Correlations.scala\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/SimpleFPGrowth.scala\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/Word2VecExample.scala\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/RankingMetricsExample.scala\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/NaiveBayesExample.scala\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/IsotonicRegressionExample.scala\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/LinearRegression.scala\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/MultivariateSummarizer.scala\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/TallSkinnySVD.scala\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/FPGrowthExample.scala\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/SVMWithSGDExample.scala\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/RecommendationExample.scala\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/PCAOnSourceVectorExample.scala\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/MovieLensALS.scala\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/DecisionTreeClassificationExample.scala\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/KMeansExample.scala\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/PMMLModelExportExample.scala\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/LogisticRegressionWithLBFGSExample.scala\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/StratifiedSamplingExample.scala\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/CosineSimilarity.scala\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/GradientBoostingRegressionExample.scala\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/SummaryStatisticsExample.scala\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/StreamingLogisticRegression.scala\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/RegressionMetricsExample.scala\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/DenseKMeans.scala\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/HypothesisTestingKolmogorovSmirnovTestExample.scala\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/RandomForestClassificationExample.scala\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/LDAExample.scala\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/DecisionTreeRegressionExample.scala\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/PCAExample.scala\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/KernelDensityEstimationExample.scala\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/MulticlassMetricsExample.scala\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/BinaryClassification.scala\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/PowerIterationClusteringExample.scala\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/SVDExample.scala\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/StreamingKMeansExample.scala\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/BisectingKMeansExample.scala\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/MultiLabelMetricsExample.scala\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/BinaryClassificationMetricsExample.scala\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/RandomRDDGeneration.scala\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/AssociationRulesExample.scala\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/HypothesisTestingExample.scala\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/TFIDFExample.scala\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/LinearRegressionWithSGDExample.scala\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/PCAOnRowMatrixExample.scala\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/NormalizerExample.scala\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/SparseNaiveBayes.scala\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/ChiSqSelectorExample.scala\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/CorrelationsExample.scala\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/RandomForestRegressionExample.scala\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/AbstractParams.scala\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/SkewedGroupByTest.scala\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/SparkPi.scala\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/OneVsRestExample.scala\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/DecisionTreeExample.scala\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/CountVectorizerExample.scala\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/ImputerExample.scala\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/StandardScalerExample.scala\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/GaussianMixtureExample.scala\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/BucketedRandomProjectionLSHExample.scala\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/ElementwiseProductExample.scala\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/CorrelationExample.scala\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/ModelSelectionViaCrossValidationExample.scala\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/ModelSelectionViaTrainValidationSplitExample.scala\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/RandomForestRegressorExample.scala\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/MultilayerPerceptronClassifierExample.scala\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/LogisticRegressionWithElasticNetExample.scala\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/LogisticRegressionExample.scala\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/VectorSizeHintExample.scala\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/SummarizerExample.scala\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/PrefixSpanExample.scala\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/ALSExample.scala\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/MinMaxScalerExample.scala\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/Word2VecExample.scala\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/InteractionExample.scala\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/NaiveBayesExample.scala\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/IsotonicRegressionExample.scala\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/StringIndexerExample.scala\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/ChiSquareTestExample.scala\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/FPGrowthExample.scala\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/DecisionTreeClassificationExample.scala\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/KMeansExample.scala\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/StopWordsRemoverExample.scala\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/FeatureHasherExample.scala\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/LinearRegressionWithElasticNetExample.scala\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/LogisticRegressionSummaryExample.scala\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/IndexToStringExample.scala\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/GradientBoostedTreeClassifierExample.scala\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/GradientBoostedTreeRegressorExample.scala\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/SQLTransformerExample.scala\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/LDAExample.scala\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/DecisionTreeRegressionExample.scala\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/LinearSVCExample.scala\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/TfIdfExample.scala\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/PCAExample.scala\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/MulticlassLogisticRegressionWithElasticNetExample.scala\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/GeneralizedLinearRegressionExample.scala\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/MinHashLSHExample.scala\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/QuantileDiscretizerExample.scala\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/DeveloperApiExample.scala\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/VectorIndexerExample.scala\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/LinearRegressionExample.scala\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/DCTExample.scala\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/PolynomialExpansionExample.scala\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/OneHotEncoderEstimatorExample.scala\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/BinarizerExample.scala\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/PowerIterationClusteringExample.scala\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/DataFrameExample.scala\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/RandomForestExample.scala\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/MaxAbsScalerExample.scala\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/RFormulaExample.scala\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/TokenizerExample.scala\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/VectorSlicerExample.scala\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/RandomForestClassifierExample.scala\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/NGramExample.scala\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/BisectingKMeansExample.scala\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/PipelineExample.scala\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/VectorAssemblerExample.scala\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/AFTSurvivalRegressionExample.scala\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/BucketizerExample.scala\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/UnaryTransformerExample.scala\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/GBTExample.scala\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/NormalizerExample.scala\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/EstimatorTransformerParamExample.scala\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/ChiSqSelectorExample.scala\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/CustomReceiver.scala\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/SqlNetworkWordCount.scala\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/RawNetworkGrep.scala\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/StatefulNetworkWordCount.scala\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/RecoverableNetworkWordCount.scala\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/clickstream/\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/clickstream/PageViewGenerator.scala\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/clickstream/PageViewStream.scala\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/DirectKafkaWordCount.scala\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/HdfsWordCount.scala\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/QueueStream.scala\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/StreamingExamples.scala\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/NetworkWordCount.scala\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/SparkKMeans.scala\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/DFSReadWriteTest.scala\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ExceptionHandlingTest.scala\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/SparkALS.scala\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/LogQuery.scala\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/LocalLR.scala\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/SparkSQLExample.scala\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/hive/\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/hive/SparkHiveExample.scala\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/RDDRelation.scala\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/streaming/\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/streaming/StructuredNetworkWordCount.scala\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/streaming/StructuredNetworkWordCountWindowed.scala\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/streaming/StructuredKafkaWordCount.scala\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/streaming/StructuredSessionization.scala\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/UserDefinedUntypedAggregation.scala\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/UserDefinedTypedAggregation.scala\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/SQLDataSourceExample.scala\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/MultiBroadcastTest.scala\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/LocalALS.scala\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/DriverSubmissionTest.scala\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/LocalFileLR.scala\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/SimpleSkewedGroupByTest.scala\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/GroupByTest.scala\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/SparkRemoteFileTest.scala\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/java/\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/java/org/\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/java/org/apache/\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/JavaSparkPi.java\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/JavaTC.java\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/JavaHdfsLR.java\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/JavaStatusTrackerDemo.java\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaGradientBoostingClassificationExample.java\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaHypothesisTestingExample.java\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaElementwiseProductExample.java\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaGaussianMixtureExample.java\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaAssociationRulesExample.java\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaDecisionTreeRegressionExample.java\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaBinaryClassificationMetricsExample.java\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaLBFGSExample.java\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaPowerIterationClusteringExample.java\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaMulticlassClassificationMetricsExample.java\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaRankingMetricsExample.java\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaRecommendationExample.java\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaHypothesisTestingKolmogorovSmirnovTestExample.java\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaRegressionMetricsExample.java\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaSVDExample.java\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaRandomForestClassificationExample.java\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaRandomForestRegressionExample.java\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaSummaryStatisticsExample.java\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaGradientBoostingRegressionExample.java\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaIsotonicRegressionExample.java\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaDecisionTreeClassificationExample.java\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaALS.java\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaKMeansExample.java\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaSVMWithSGDExample.java\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaLinearRegressionWithSGDExample.java\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaLatentDirichletAllocationExample.java\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaMultiLabelClassificationMetricsExample.java\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaBisectingKMeansExample.java\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaStratifiedSamplingExample.java\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaSimpleFPGrowth.java\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaLogisticRegressionWithLBFGSExample.java\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaPrefixSpanExample.java\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaPCAExample.java\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaCorrelationsExample.java\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaStreamingTestExample.java\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaNaiveBayesExample.java\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaChiSqSelectorExample.java\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaKernelDensityEstimationExample.java\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaBinarizerExample.java\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaElementwiseProductExample.java\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaInteractionExample.java\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaGaussianMixtureExample.java\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaBucketedRandomProjectionLSHExample.java\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaLogisticRegressionWithElasticNetExample.java\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaBucketizerExample.java\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaDecisionTreeRegressionExample.java\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaIndexToStringExample.java\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaCorrelationExample.java\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaMaxAbsScalerExample.java\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaGradientBoostedTreeClassifierExample.java\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaPowerIterationClusteringExample.java\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaDocument.java\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaLogisticRegressionSummaryExample.java\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaStandardScalerExample.java\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaMulticlassLogisticRegressionWithElasticNetExample.java\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaChiSquareTestExample.java\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaFPGrowthExample.java\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaQuantileDiscretizerExample.java\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaPolynomialExpansionExample.java\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaTokenizerExample.java\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaFeatureHasherExample.java\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaVectorIndexerExample.java\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaVectorSlicerExample.java\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaStringIndexerExample.java\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaIsotonicRegressionExample.java\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaMultilayerPerceptronClassifierExample.java\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaOneVsRestExample.java\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaRFormulaExample.java\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaDecisionTreeClassificationExample.java\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaLDAExample.java\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaDCTExample.java\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaKMeansExample.java\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaModelSelectionViaCrossValidationExample.java\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaSummarizerExample.java\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaRandomForestClassifierExample.java\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaImputerExample.java\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaMinMaxScalerExample.java\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaEstimatorTransformerParamExample.java\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaOneHotEncoderEstimatorExample.java\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaSQLTransformerExample.java\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaLabeledDocument.java\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaStopWordsRemoverExample.java\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaTfIdfExample.java\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaMinHashLSHExample.java\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaBisectingKMeansExample.java\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaLinearRegressionWithElasticNetExample.java\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaNGramExample.java\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaModelSelectionViaTrainValidationSplitExample.java\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaPrefixSpanExample.java\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaWord2VecExample.java\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaGradientBoostedTreeRegressorExample.java\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaAFTSurvivalRegressionExample.java\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaPCAExample.java\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaCountVectorizerExample.java\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaGeneralizedLinearRegressionExample.java\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaVectorSizeHintExample.java\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaNaiveBayesExample.java\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaNormalizerExample.java\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaChiSqSelectorExample.java\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaVectorAssemblerExample.java\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaRandomForestRegressorExample.java\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaLinearSVCExample.java\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaPipelineExample.java\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaALSExample.java\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/streaming/\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/streaming/JavaStatefulNetworkWordCount.java\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/streaming/JavaNetworkWordCount.java\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/streaming/JavaCustomReceiver.java\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/streaming/JavaRecoverableNetworkWordCount.java\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/streaming/JavaDirectKafkaWordCount.java\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/streaming/JavaQueueStream.java\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/streaming/JavaSqlNetworkWordCount.java\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/streaming/JavaRecord.java\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/sql/\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/sql/JavaSparkSQLExample.java\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/sql/hive/\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/sql/hive/JavaSparkHiveExample.java\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/sql/JavaUserDefinedTypedAggregation.java\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/sql/JavaUserDefinedUntypedAggregation.java\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/sql/streaming/\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/sql/streaming/JavaStructuredKafkaWordCount.java\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/sql/streaming/JavaStructuredNetworkWordCount.java\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/sql/streaming/JavaStructuredNetworkWordCountWindowed.java\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/sql/streaming/JavaStructuredSessionization.java\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/sql/JavaSQLDataSourceExample.java\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/JavaWordCount.java\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/JavaLogQuery.java\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/JavaPageRank.java\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/r/\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/r/RSparkSQLExample.R\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/r/ml/\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/r/ml/glm.R\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/r/ml/als.R\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/r/ml/kmeans.R\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/r/ml/mlp.R\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/r/ml/lda.R\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/r/ml/ml.R\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/r/ml/naiveBayes.R\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/r/ml/kstest.R\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/r/ml/gbt.R\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/r/ml/survreg.R\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/r/ml/svmLinear.R\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/r/ml/bisectingKmeans.R\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/r/ml/gaussianMixture.R\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/r/ml/isoreg.R\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/r/ml/randomForest.R\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/r/ml/decisionTree.R\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/r/ml/logit.R\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/r/ml/fpm.R\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/r/data-manipulation.R\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/r/streaming/\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/r/streaming/structured_network_wordcount.R\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/r/dataframe.R\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/resources/\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/resources/people.csv\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/resources/users.avro\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/resources/employees.json\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/resources/full_user.avsc\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/resources/people.json\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/resources/people.txt\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/resources/kv1.txt\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/resources/users.parquet\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/resources/users.orc\n",
      "spark-2.4.0-bin-hadoop2.7/examples/src/main/resources/user.avsc\n",
      "spark-2.4.0-bin-hadoop2.7/NOTICE\n",
      "spark-2.4.0-bin-hadoop2.7/data/\n",
      "spark-2.4.0-bin-hadoop2.7/data/graphx/\n",
      "spark-2.4.0-bin-hadoop2.7/data/graphx/users.txt\n",
      "spark-2.4.0-bin-hadoop2.7/data/graphx/followers.txt\n",
      "spark-2.4.0-bin-hadoop2.7/data/mllib/\n",
      "spark-2.4.0-bin-hadoop2.7/data/mllib/gmm_data.txt\n",
      "spark-2.4.0-bin-hadoop2.7/data/mllib/images/\n",
      "spark-2.4.0-bin-hadoop2.7/data/mllib/images/origin/\n",
      "spark-2.4.0-bin-hadoop2.7/data/mllib/images/origin/multi-channel/\n",
      "spark-2.4.0-bin-hadoop2.7/data/mllib/images/origin/multi-channel/chr30.4.184.jpg\n",
      "spark-2.4.0-bin-hadoop2.7/data/mllib/images/origin/multi-channel/grayscale.jpg\n",
      "spark-2.4.0-bin-hadoop2.7/data/mllib/images/origin/multi-channel/BGRA_alpha_60.png\n",
      "spark-2.4.0-bin-hadoop2.7/data/mllib/images/origin/multi-channel/BGRA.png\n",
      "spark-2.4.0-bin-hadoop2.7/data/mllib/images/origin/kittens/\n",
      "spark-2.4.0-bin-hadoop2.7/data/mllib/images/origin/kittens/DP153539.jpg\n",
      "spark-2.4.0-bin-hadoop2.7/data/mllib/images/origin/kittens/54893.jpg\n",
      "spark-2.4.0-bin-hadoop2.7/data/mllib/images/origin/kittens/not-image.txt\n",
      "spark-2.4.0-bin-hadoop2.7/data/mllib/images/origin/kittens/DP802813.jpg\n",
      "spark-2.4.0-bin-hadoop2.7/data/mllib/images/origin/kittens/29.5.a_b_EGDP022204.jpg\n",
      "spark-2.4.0-bin-hadoop2.7/data/mllib/images/origin/license.txt\n",
      "spark-2.4.0-bin-hadoop2.7/data/mllib/images/license.txt\n",
      "spark-2.4.0-bin-hadoop2.7/data/mllib/images/partitioned/\n",
      "spark-2.4.0-bin-hadoop2.7/data/mllib/images/partitioned/cls=multichannel/\n",
      "spark-2.4.0-bin-hadoop2.7/data/mllib/images/partitioned/cls=multichannel/date=2018-02/\n",
      "spark-2.4.0-bin-hadoop2.7/data/mllib/images/partitioned/cls=multichannel/date=2018-02/chr30.4.184.jpg\n",
      "spark-2.4.0-bin-hadoop2.7/data/mllib/images/partitioned/cls=multichannel/date=2018-02/grayscale.jpg\n",
      "spark-2.4.0-bin-hadoop2.7/data/mllib/images/partitioned/cls=multichannel/date=2018-01/\n",
      "spark-2.4.0-bin-hadoop2.7/data/mllib/images/partitioned/cls=multichannel/date=2018-01/BGRA_alpha_60.png\n",
      "spark-2.4.0-bin-hadoop2.7/data/mllib/images/partitioned/cls=multichannel/date=2018-01/BGRA.png\n",
      "spark-2.4.0-bin-hadoop2.7/data/mllib/images/partitioned/cls=kittens/\n",
      "spark-2.4.0-bin-hadoop2.7/data/mllib/images/partitioned/cls=kittens/date=2018-02/\n",
      "spark-2.4.0-bin-hadoop2.7/data/mllib/images/partitioned/cls=kittens/date=2018-02/DP153539.jpg\n",
      "spark-2.4.0-bin-hadoop2.7/data/mllib/images/partitioned/cls=kittens/date=2018-02/54893.jpg\n",
      "spark-2.4.0-bin-hadoop2.7/data/mllib/images/partitioned/cls=kittens/date=2018-02/DP802813.jpg\n",
      "spark-2.4.0-bin-hadoop2.7/data/mllib/images/partitioned/cls=kittens/date=2018-01/\n",
      "spark-2.4.0-bin-hadoop2.7/data/mllib/images/partitioned/cls=kittens/date=2018-01/not-image.txt\n",
      "spark-2.4.0-bin-hadoop2.7/data/mllib/images/partitioned/cls=kittens/date=2018-01/29.5.a_b_EGDP022204.jpg\n",
      "spark-2.4.0-bin-hadoop2.7/data/mllib/sample_linear_regression_data.txt\n",
      "spark-2.4.0-bin-hadoop2.7/data/mllib/sample_lda_data.txt\n",
      "spark-2.4.0-bin-hadoop2.7/data/mllib/kmeans_data.txt\n",
      "spark-2.4.0-bin-hadoop2.7/data/mllib/sample_svm_data.txt\n",
      "spark-2.4.0-bin-hadoop2.7/data/mllib/streaming_kmeans_data_test.txt\n",
      "spark-2.4.0-bin-hadoop2.7/data/mllib/sample_kmeans_data.txt\n",
      "spark-2.4.0-bin-hadoop2.7/data/mllib/sample_multiclass_classification_data.txt\n",
      "spark-2.4.0-bin-hadoop2.7/data/mllib/sample_fpgrowth.txt\n",
      "spark-2.4.0-bin-hadoop2.7/data/mllib/pic_data.txt\n",
      "spark-2.4.0-bin-hadoop2.7/data/mllib/pagerank_data.txt\n",
      "spark-2.4.0-bin-hadoop2.7/data/mllib/sample_isotonic_regression_libsvm_data.txt\n",
      "spark-2.4.0-bin-hadoop2.7/data/mllib/als/\n",
      "spark-2.4.0-bin-hadoop2.7/data/mllib/als/test.data\n",
      "spark-2.4.0-bin-hadoop2.7/data/mllib/als/sample_movielens_ratings.txt\n",
      "spark-2.4.0-bin-hadoop2.7/data/mllib/sample_lda_libsvm_data.txt\n",
      "spark-2.4.0-bin-hadoop2.7/data/mllib/sample_binary_classification_data.txt\n",
      "spark-2.4.0-bin-hadoop2.7/data/mllib/ridge-data/\n",
      "spark-2.4.0-bin-hadoop2.7/data/mllib/ridge-data/lpsa.data\n",
      "spark-2.4.0-bin-hadoop2.7/data/mllib/sample_movielens_data.txt\n",
      "spark-2.4.0-bin-hadoop2.7/data/mllib/iris_libsvm.txt\n",
      "spark-2.4.0-bin-hadoop2.7/data/mllib/sample_libsvm_data.txt\n",
      "spark-2.4.0-bin-hadoop2.7/data/streaming/\n",
      "spark-2.4.0-bin-hadoop2.7/data/streaming/AFINN-111.txt\n",
      "spark-2.4.0-bin-hadoop2.7/README.md\n",
      "spark-2.4.0-bin-hadoop2.7/LICENSE\n"
     ]
    }
   ],
   "source": [
    "# !apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
    "# !wget -q http://archive.apache.org/dist/spark/spark-2.4.0/spark-2.4.0-bin-hadoop2.7.tgz\n",
    "# !tar -xvf spark-2.4.0-bin-hadoop2.7.tgz\n",
    "# !pip install -q findspark\n",
    "# import os\n",
    "# os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "# os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.0-bin-hadoop2.7\"\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4722,
     "status": "ok",
     "timestamp": 1643031600838,
     "user": {
      "displayName": "Quỳnh Tôn",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "17019808029748888744"
     },
     "user_tz": -420
    },
    "id": "-pqRnRr_83KH",
    "outputId": "483bcd6a-4e0d-43b6-d00c-97ed919f71fc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/gdrive\n"
     ]
    }
   ],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/gdrive', force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1643031600839,
     "user": {
      "displayName": "Quỳnh Tôn",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "17019808029748888744"
     },
     "user_tz": -420
    },
    "id": "D-iOMV-r83SI",
    "outputId": "9281d27f-f879-4c8c-b264-727345f18db3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/gdrive/My Drive/LDS9_K272_ONLINE_TonNuKhanhQuynh\n"
     ]
    }
   ],
   "source": [
    "# %cd '/content/gdrive/My Drive/LDS9_K272_ONLINE_TonNuKhanhQuynh/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1643031600840,
     "user": {
      "displayName": "Quỳnh Tôn",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "17019808029748888744"
     },
     "user_tz": -420
    },
    "id": "Af2VIf6w80Te"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import Tokenizer, RegexTokenizer\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "from pyspark.ml.feature import IDF, Tokenizer\n",
    "from pyspark.ml.feature import NGram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 4815,
     "status": "ok",
     "timestamp": 1643031605648,
     "user": {
      "displayName": "Quỳnh Tôn",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "17019808029748888744"
     },
     "user_tz": -420
    },
    "id": "uDj5JqrQ80Tf"
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('nlp_1').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 24,
     "status": "ok",
     "timestamp": 1643031605649,
     "user": {
      "displayName": "Quỳnh Tôn",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "17019808029748888744"
     },
     "user_tz": -420
    },
    "id": "pwTjUuUJ80Tg"
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_ZVBjz9t80Tg"
   },
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9665,
     "status": "ok",
     "timestamp": 1643031615292,
     "user": {
      "displayName": "Quỳnh Tôn",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "17019808029748888744"
     },
     "user_tz": -420
    },
    "id": "9rSHnmun80Ti",
    "outputId": "0bd4b801-286f-49d4-e160-7ff27c2db57f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+---+--------------------+--------------------+------+---------------+-----------------------+-------------+---------------+----------+\n",
      "|Unnamed: 0|Clothing ID|Age|               Title|         Review Text|Rating|Recommended IND|Positive Feedback Count|Division Name|Department Name|Class Name|\n",
      "+----------+-----------+---+--------------------+--------------------+------+---------------+-----------------------+-------------+---------------+----------+\n",
      "|         0|        767| 33|                 NaN|Absolutely wonder...|     4|              1|                      0|    Initmates|       Intimate| Intimates|\n",
      "|         1|       1080| 34|                 NaN|Love this dress! ...|     5|              1|                      4|      General|        Dresses|   Dresses|\n",
      "|         2|       1077| 60|Some major design...|I had such high h...|     3|              0|                      0|      General|        Dresses|   Dresses|\n",
      "+----------+-----------+---+--------------------+--------------------+------+---------------+-----------------------+-------------+---------------+----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_excel('Du lieu cung cap/womens-ecommerce-clothing-reviews/Womens_Clothing_E_Commerce_Reviews.xlsx')\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "df_schema = StructType([StructField(\"Unnamed: 0\", IntegerType(), True)\\\n",
    "                       ,StructField(\"Clothing ID\", IntegerType(), True)\\\n",
    "                       ,StructField(\"Age\", IntegerType(), True)\\\n",
    "                       ,StructField(\"Title\", StringType(), True)\\\n",
    "                       ,StructField(\"Review Text\", StringType(), True)\\\n",
    "                       ,StructField(\"Rating\", IntegerType(), True)\\\n",
    "                       ,StructField(\"Recommended IND\", IntegerType(), True)\\\n",
    "                       ,StructField(\"Positive Feedback Count\", IntegerType(), True)\\\n",
    "                       ,StructField(\"Division Name\", StringType(), True)\\\n",
    "                       ,StructField(\"Department Name\", StringType(), True)\\\n",
    "                       ,StructField(\"Class Name\", StringType(), True)])\n",
    "\n",
    "\n",
    "df = spark.createDataFrame(df, schema=df_schema)\n",
    "df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1023,
     "status": "ok",
     "timestamp": 1643031616290,
     "user": {
      "displayName": "Quỳnh Tôn",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "17019808029748888744"
     },
     "user_tz": -420
    },
    "id": "VPdMyEjJ80Tk",
    "outputId": "f1735a3e-7674-4417-d90d-10d79d954381"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23481"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1643031616291,
     "user": {
      "displayName": "Quỳnh Tôn",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "17019808029748888744"
     },
     "user_tz": -420
    },
    "id": "MytyLLFP80Tl",
    "outputId": "e30c730b-3e55-4366-f397-79b225ad7dd4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Unnamed: 0: integer (nullable = true)\n",
      " |-- Clothing ID: integer (nullable = true)\n",
      " |-- Age: integer (nullable = true)\n",
      " |-- Title: string (nullable = true)\n",
      " |-- Review Text: string (nullable = true)\n",
      " |-- Rating: integer (nullable = true)\n",
      " |-- Recommended IND: integer (nullable = true)\n",
      " |-- Positive Feedback Count: integer (nullable = true)\n",
      " |-- Division Name: string (nullable = true)\n",
      " |-- Department Name: string (nullable = true)\n",
      " |-- Class Name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1643031616292,
     "user": {
      "displayName": "Quỳnh Tôn",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "17019808029748888744"
     },
     "user_tz": -420
    },
    "id": "VQwUkGuO80Tm"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2548,
     "status": "ok",
     "timestamp": 1643031618833,
     "user": {
      "displayName": "Quỳnh Tôn",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "17019808029748888744"
     },
     "user_tz": -420
    },
    "id": "he37nBCU80Tm",
    "outputId": "fe635f14-35ca-4190-83a1-2c57c2ea52dc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+---+-----+-----------+------+---------------+-----------------------+-------------+---------------+----------+\n",
      "|Unnamed: 0|Clothing ID|Age|Title|Review Text|Rating|Recommended IND|Positive Feedback Count|Division Name|Department Name|Class Name|\n",
      "+----------+-----------+---+-----+-----------+------+---------------+-----------------------+-------------+---------------+----------+\n",
      "|         0|          0|  0| 3810|        845|     0|              0|                      0|           14|             14|        14|\n",
      "+----------+-----------+---+-----+-----------+------+---------------+-----------------------+-------------+---------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in df.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 911,
     "status": "ok",
     "timestamp": 1643031619719,
     "user": {
      "displayName": "Quỳnh Tôn",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "17019808029748888744"
     },
     "user_tz": -420
    },
    "id": "aBpivC8E80Tn",
    "outputId": "91b319fc-53a5-435b-b214-912fef91569f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: 23481\n",
      "After: 19670\n"
     ]
    }
   ],
   "source": [
    "print('Before:', df.count())\n",
    "df = df.where(~isnan('Title'))\n",
    "df = df.where(~isnan('Review Text'))\n",
    "# df = df.where(~isnan('Division Name'))\n",
    "# df = df.where(~isnan('Department Name'))\n",
    "# df = df.where(~isnan('Class Name'))\n",
    "print('After:', df.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eDH2-05l80To"
   },
   "source": [
    "### Clean and prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 34,
     "status": "ok",
     "timestamp": 1643031619720,
     "user": {
      "displayName": "Quỳnh Tôn",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "17019808029748888744"
     },
     "user_tz": -420
    },
    "id": "pXL0TdLE80To"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 34,
     "status": "ok",
     "timestamp": 1643031619721,
     "user": {
      "displayName": "Quỳnh Tôn",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "17019808029748888744"
     },
     "user_tz": -420
    },
    "id": "LHdnXfni80Tp"
   },
   "outputs": [],
   "source": [
    "# # Convert categorical strings to index values\n",
    "# indexer1 = StringIndexer(inputCol='Division Name', outputCol='Division Name_idx')\n",
    "# indexer2 = StringIndexer(inputCol='Department Name', outputCol='Department Name_idx')\n",
    "# indexer3 = StringIndexer(inputCol='Class Name', outputCol='Class Name_idx')\n",
    "\n",
    "\n",
    "# #One-hot encode index values\n",
    "# onehot1 = OneHotEncoder(\n",
    "#         inputCols = ['Division Name_idx'],\n",
    "#         outputCols = ['Division Name_dummy'])\n",
    "\n",
    "# onehot2 = OneHotEncoder(\n",
    "#         inputCols = ['Department Name_idx'],\n",
    "#         outputCols = ['Department Name_dummy'])\n",
    "\n",
    "# onehot3 = OneHotEncoder(\n",
    "#         inputCols = ['Class Name_idx'],\n",
    "#         outputCols = ['Class Name_dummy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "executionInfo": {
     "elapsed": 32,
     "status": "ok",
     "timestamp": 1643031619723,
     "user": {
      "displayName": "Quỳnh Tôn",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "17019808029748888744"
     },
     "user_tz": -420
    },
    "id": "nTF_XNml80Tp"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover\n",
    "from pyspark.ml.feature import CountVectorizer, IDF, StringIndexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "executionInfo": {
     "elapsed": 661,
     "status": "ok",
     "timestamp": 1643031656299,
     "user": {
      "displayName": "Quỳnh Tôn",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "17019808029748888744"
     },
     "user_tz": -420
    },
    "id": "q8wLcyjh80Tp"
   },
   "outputs": [],
   "source": [
    "# rating_to_class = StringIndexer(inputCol='Rating',outputCol='class')\n",
    "class_to_num = StringIndexer(inputCol = 'Rating',outputCol = 'label')\n",
    "tokenizer = Tokenizer(inputCol='Review Text',outputCol='token_Review Text')\n",
    "stopremove = StopWordsRemover(inputCol='token_Review Text', outputCol=\"stop_tokens\")\n",
    "count_vec=CountVectorizer(inputCol='stop_tokens',outputCol='c_vec')\n",
    "idf= IDF(inputCol='c_vec', outputCol='tf_idf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "executionInfo": {
     "elapsed": 348,
     "status": "ok",
     "timestamp": 1643031660043,
     "user": {
      "displayName": "Quỳnh Tôn",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "17019808029748888744"
     },
     "user_tz": -420
    },
    "id": "A6v_ePIE80Tq"
   },
   "outputs": [],
   "source": [
    "tokenizer1 = Tokenizer(inputCol='Title',outputCol='token_Title')\n",
    "stopremove1 = StopWordsRemover(inputCol='token_Title', outputCol=\"stop_tokens_Title\")\n",
    "count_vec1=CountVectorizer(inputCol='stop_tokens_Title',outputCol='c_vec_Title')\n",
    "idf1= IDF(inputCol='c_vec_Title', outputCol='tf_idf1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1643031661474,
     "user": {
      "displayName": "Quỳnh Tôn",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "17019808029748888744"
     },
     "user_tz": -420
    },
    "id": "GwOeu27R80Tq"
   },
   "outputs": [],
   "source": [
    "# Assemble predictors into a single column\n",
    "clean_up = VectorAssembler(inputCols=['tf_idf1','tf_idf','Recommended IND'], \n",
    "                             outputCol='features')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PCDcHgqX80Tr"
   },
   "source": [
    "### The model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "executionInfo": {
     "elapsed": 341,
     "status": "ok",
     "timestamp": 1643031663844,
     "user": {
      "displayName": "Quỳnh Tôn",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "17019808029748888744"
     },
     "user_tz": -420
    },
    "id": "gSocpcDD80Tr"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import NaiveBayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1643031664692,
     "user": {
      "displayName": "Quỳnh Tôn",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "17019808029748888744"
     },
     "user_tz": -420
    },
    "id": "K0v0fNeL80Tr"
   },
   "outputs": [],
   "source": [
    "nb = NaiveBayes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L1cNdFrE80Tu"
   },
   "source": [
    "### Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1643031666026,
     "user": {
      "displayName": "Quỳnh Tôn",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "17019808029748888744"
     },
     "user_tz": -420
    },
    "id": "dVy6pVfg80Tu"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1643031666026,
     "user": {
      "displayName": "Quỳnh Tôn",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "17019808029748888744"
     },
     "user_tz": -420
    },
    "id": "yWyAnbVN80Tv"
   },
   "outputs": [],
   "source": [
    "data_prep_pipe = Pipeline(stages=[\n",
    "                                  class_to_num,\n",
    "                                  tokenizer,\n",
    "                                  stopremove,\n",
    "                                  count_vec,\n",
    "                                  idf,\n",
    "                                  tokenizer1,\n",
    "                                  stopremove1,\n",
    "                                  count_vec1,\n",
    "                                  idf1,\n",
    "                                  clean_up])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "executionInfo": {
     "elapsed": 13622,
     "status": "ok",
     "timestamp": 1643031679645,
     "user": {
      "displayName": "Quỳnh Tôn",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "17019808029748888744"
     },
     "user_tz": -420
    },
    "id": "R3mooCt680Tv"
   },
   "outputs": [],
   "source": [
    "cleaner = data_prep_pipe.fit(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "executionInfo": {
     "elapsed": 1145,
     "status": "ok",
     "timestamp": 1643031680764,
     "user": {
      "displayName": "Quỳnh Tôn",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "17019808029748888744"
     },
     "user_tz": -420
    },
    "id": "gW7Dsl9-80Tv"
   },
   "outputs": [],
   "source": [
    "clean_data = cleaner.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 973,
     "status": "ok",
     "timestamp": 1643031681734,
     "user": {
      "displayName": "Quỳnh Tôn",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "17019808029748888744"
     },
     "user_tz": -420
    },
    "id": "clIAy36BBBdi",
    "outputId": "7f8dc74e-d2e9-4fdd-bdc4-9772ca4d8110"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+---+--------------------+--------------------+------+---------------+-----------------------+--------------+---------------+----------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|Unnamed: 0|Clothing ID|Age|               Title|         Review Text|Rating|Recommended IND|Positive Feedback Count| Division Name|Department Name|Class Name|label|   token_Review Text|         stop_tokens|               c_vec|              tf_idf|         token_Title|   stop_tokens_Title|         c_vec_Title|             tf_idf1|            features|\n",
      "+----------+-----------+---+--------------------+--------------------+------+---------------+-----------------------+--------------+---------------+----------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|         2|       1077| 60|Some major design...|I had such high h...|     3|              0|                      0|       General|        Dresses|   Dresses|  2.0|[i, had, such, hi...|[high, hopes, dre...|(35483,[2,5,7,9,1...|(35483,[2,5,7,9,1...|[some, major, des...|[major, design, f...|(5910,[45,474,116...|(5910,[45,474,116...|(41394,[45,474,11...|\n",
      "|         3|       1049| 50|    My favorite buy!|I love, love, lov...|     5|              1|                      0|General Petite|        Bottoms|     Pants|  0.0|[i, love,, love,,...|[love,, love,, lo...|(35483,[1,6,8,40,...|(35483,[1,6,8,40,...|[my, favorite, buy!]|    [favorite, buy!]|(5910,[43,471],[1...|(5910,[43,471],[4...|(41394,[43,471,59...|\n",
      "|         4|        847| 47|    Flattering shirt|This shirt is ver...|     5|              1|                      6|       General|           Tops|   Blouses|  0.0|[this, shirt, is,...|[shirt, flatterin...|(35483,[1,6,23,35...|(35483,[1,6,23,35...| [flattering, shirt]| [flattering, shirt]|(5910,[13,26],[1....|(5910,[13,26],[3....|(41394,[13,26,591...|\n",
      "|         5|       1080| 49|Not for the very ...|I love tracy rees...|     2|              0|                      4|       General|        Dresses|   Dresses|  3.0|[i, love, tracy, ...|[love, tracy, ree...|(35483,[1,2,6,14,...|(35483,[1,2,6,14,...|[not, for, the, v...|            [petite]|  (5910,[100],[1.0])|(5910,[100],[5.55...|(41394,[100,5911,...|\n",
      "|         6|        858| 39|Cagrcoal shimmer fun|I aded this in my...|     5|              1|                      1|General Petite|           Tops|     Knits|  0.0|[i, aded, this, i...|[aded, basket, ht...|(35483,[3,4,9,12,...|(35483,[3,4,9,12,...|[cagrcoal, shimme...|[cagrcoal, shimme...|(5910,[22,1037,47...|(5910,[22,1037,47...|(41394,[22,1037,4...|\n",
      "+----------+-----------+---+--------------------+--------------------+------+---------------+-----------------------+--------------+---------------+----------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clean_data.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a_SuewIT80Tw"
   },
   "source": [
    "### Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1643031681736,
     "user": {
      "displayName": "Quỳnh Tôn",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "17019808029748888744"
     },
     "user_tz": -420
    },
    "id": "UuNcnz4O80Tw"
   },
   "outputs": [],
   "source": [
    "clean_data = clean_data.select(['label','features'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1643031681737,
     "user": {
      "displayName": "Quỳnh Tôn",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "17019808029748888744"
     },
     "user_tz": -420
    },
    "id": "mQ9aT6jw80Tw",
    "outputId": "26f26cc2-f973-4095-b67a-2ad1ed5dfafc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label|            features|\n",
      "+-----+--------------------+\n",
      "|  2.0|(41394,[45,474,11...|\n",
      "|  0.0|(41394,[43,471,59...|\n",
      "|  0.0|(41394,[13,26,591...|\n",
      "|  3.0|(41394,[100,5911,...|\n",
      "|  0.0|(41394,[22,1037,4...|\n",
      "|  1.0|(41394,[253,258,4...|\n",
      "|  0.0|(41394,[13,5911,5...|\n",
      "|  0.0|(41394,[20,22,591...|\n",
      "|  0.0|(41394,[2143,5912...|\n",
      "|  0.0|(41394,[9,40,5910...|\n",
      "+-----+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clean_data.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SFeF-MYh80Tr"
   },
   "source": [
    "### Resample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "executionInfo": {
     "elapsed": 552,
     "status": "ok",
     "timestamp": 1643031682279,
     "user": {
      "displayName": "Quỳnh Tôn",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "17019808029748888744"
     },
     "user_tz": -420
    },
    "id": "yzr9U2h_80Ts"
   },
   "outputs": [],
   "source": [
    "df0 = clean_data.filter(col(\"label\") == 0)\n",
    "df1 = clean_data.filter(col(\"label\") == 1)\n",
    "df2 = clean_data.filter(col(\"label\") == 2)\n",
    "df3 = clean_data.filter(col(\"label\") == 3)\n",
    "df4 = clean_data.filter(col(\"label\") == 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1643031682280,
     "user": {
      "displayName": "Quỳnh Tôn",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "17019808029748888744"
     },
     "user_tz": -420
    },
    "id": "uip8O9Ot80Ts"
   },
   "outputs": [],
   "source": [
    "def resample(df,combined_df):\n",
    "    ratio = int(df0.count()/df.count())\n",
    "    a = range(ratio)\n",
    "    # duplicate the minority rows\n",
    "    oversampled = df.withColumn(\"dummy\", explode(array([lit(x) for x in a])))\\\n",
    "                                                                       .drop('dummy')\n",
    "    # combine both oversampled minority rows and previous majority rows \n",
    "    combined_df = combined_df.unionAll(oversampled)\n",
    "    return combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4605,
     "status": "ok",
     "timestamp": 1643031686881,
     "user": {
      "displayName": "Quỳnh Tôn",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "17019808029748888744"
     },
     "user_tz": -420
    },
    "id": "XOqSzlMz80Ts",
    "outputId": "f393ba7f-d971-4fe2-fd2b-48abcbf91812"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|label|count|\n",
      "+-----+-----+\n",
      "|  0.0|10870|\n",
      "|  1.0| 8576|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "combined_df = df0\n",
    "combined_df=resample(df1,combined_df)\n",
    "combined_df.groupBy(\"label\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3954,
     "status": "ok",
     "timestamp": 1643031690828,
     "user": {
      "displayName": "Quỳnh Tôn",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "17019808029748888744"
     },
     "user_tz": -420
    },
    "id": "Xgn-z4Pp80Tt",
    "outputId": "bd09740f-e113-48d0-ea10-f55ce511cc47"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|label|count|\n",
      "+-----+-----+\n",
      "|  0.0|10870|\n",
      "|  1.0| 8576|\n",
      "|  2.0| 9852|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "combined_df=resample(df2,combined_df)\n",
    "combined_df.groupBy(\"label\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4145,
     "status": "ok",
     "timestamp": 1643031694961,
     "user": {
      "displayName": "Quỳnh Tôn",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "17019808029748888744"
     },
     "user_tz": -420
    },
    "id": "9KLILRRr80Tt",
    "outputId": "7ee7a15a-bf6e-40a5-9b83-a8056194a6fa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|label|count|\n",
      "+-----+-----+\n",
      "|  0.0|10870|\n",
      "|  1.0| 8576|\n",
      "|  3.0| 9513|\n",
      "|  2.0| 9852|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "combined_df=resample(df3,combined_df)\n",
    "combined_df.groupBy(\"label\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3992,
     "status": "ok",
     "timestamp": 1643031698942,
     "user": {
      "displayName": "Quỳnh Tôn",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "17019808029748888744"
     },
     "user_tz": -420
    },
    "id": "X7JqFD8n80Tu",
    "outputId": "8ef3eacd-5d3d-4c04-b2e8-4e4639e8b9c3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|label|count|\n",
      "+-----+-----+\n",
      "|  0.0|10870|\n",
      "|  1.0| 8576|\n",
      "|  4.0|10350|\n",
      "|  3.0| 9513|\n",
      "|  2.0| 9852|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "combined_df=resample(df4,combined_df)\n",
    "combined_df.groupBy(\"label\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1643031698943,
     "user": {
      "displayName": "Quỳnh Tôn",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "17019808029748888744"
     },
     "user_tz": -420
    },
    "id": "Y2bQsV_mTKKi"
   },
   "outputs": [],
   "source": [
    "(training,testing) =  combined_df.randomSplit([0.7,0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "executionInfo": {
     "elapsed": 15987,
     "status": "ok",
     "timestamp": 1643031714922,
     "user": {
      "displayName": "Quỳnh Tôn",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "17019808029748888744"
     },
     "user_tz": -420
    },
    "id": "tvnRXXP780Tx"
   },
   "outputs": [],
   "source": [
    "rating_predictor = nb.fit(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "executionInfo": {
     "elapsed": 31,
     "status": "ok",
     "timestamp": 1643031714923,
     "user": {
      "displayName": "Quỳnh Tôn",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "17019808029748888744"
     },
     "user_tz": -420
    },
    "id": "Bkg5p3tq80Tx"
   },
   "outputs": [],
   "source": [
    "test_results = rating_predictor.transform(testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2378,
     "status": "ok",
     "timestamp": 1643031717273,
     "user": {
      "displayName": "Quỳnh Tôn",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "17019808029748888744"
     },
     "user_tz": -420
    },
    "id": "9kNPF_ml80Tx",
    "outputId": "655d74c2-dfca-4f3b-9279-abf2901d5f8c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "|label|            features|       rawPrediction|         probability|prediction|\n",
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "|  0.0|(41394,[0,2,14,14...|[-778.11312067982...|[0.99999987902997...|       0.0|\n",
      "|  0.0|(41394,[0,3,14,33...|[-185.34249998783...|[0.97785096019594...|       0.0|\n",
      "|  0.0|(41394,[0,3,22,27...|[-1769.3130700084...|[0.99999999999998...|       0.0|\n",
      "|  0.0|(41394,[0,3,33,52...|[-1374.7209100740...|[1.0,1.4640741767...|       0.0|\n",
      "|  0.0|(41394,[0,3,34,59...|[-1989.5678643891...|[0.10033448804598...|       1.0|\n",
      "|  0.0|(41394,[0,3,34,59...|[-989.31186561374...|[0.61771036413331...|       0.0|\n",
      "|  0.0|(41394,[0,3,101,5...|[-897.26879026213...|[4.75196239255606...|       1.0|\n",
      "|  0.0|(41394,[0,3,193,5...|[-1383.0816729478...|[1.0,2.6049688956...|       0.0|\n",
      "|  0.0|(41394,[0,3,235,5...|[-1124.0847158753...|[0.99999999999998...|       0.0|\n",
      "|  0.0|(41394,[0,3,5910,...|[-1694.2905971288...|[0.78086735987042...|       0.0|\n",
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_results.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 23475,
     "status": "ok",
     "timestamp": 1643031740743,
     "user": {
      "displayName": "Quỳnh Tôn",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "17019808029748888744"
     },
     "user_tz": -420
    },
    "id": "uCgT9wBG80Ty",
    "outputId": "58d83c05-4f95-4fb5-c6ca-1218d223890a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+-----+\n",
      "|label|prediction|count|\n",
      "+-----+----------+-----+\n",
      "|  2.0|       0.0|   40|\n",
      "|  1.0|       1.0| 1898|\n",
      "|  3.0|       2.0|   78|\n",
      "|  4.0|       2.0|   29|\n",
      "|  0.0|       1.0|  681|\n",
      "|  0.0|       4.0|   16|\n",
      "|  1.0|       0.0|  447|\n",
      "|  2.0|       2.0| 2666|\n",
      "|  3.0|       1.0|   15|\n",
      "|  2.0|       3.0|  114|\n",
      "|  1.0|       4.0|   15|\n",
      "|  4.0|       4.0| 3059|\n",
      "|  2.0|       4.0|   29|\n",
      "|  3.0|       4.0|   34|\n",
      "|  2.0|       1.0|  104|\n",
      "|  1.0|       2.0|  181|\n",
      "|  0.0|       0.0| 2414|\n",
      "|  1.0|       3.0|   39|\n",
      "|  4.0|       3.0|   45|\n",
      "|  0.0|       2.0|  116|\n",
      "+-----+----------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_results.groupBy(\"label\",\"prediction\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1643031740743,
     "user": {
      "displayName": "Quỳnh Tôn",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "17019808029748888744"
     },
     "user_tz": -420
    },
    "id": "LAXSsQXl80Ty"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18249,
     "status": "ok",
     "timestamp": 1643031758988,
     "user": {
      "displayName": "Quỳnh Tôn",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "17019808029748888744"
     },
     "user_tz": -420
    },
    "id": "22hLu8b080Ty",
    "outputId": "b4f985bd-b947-446e-e038-7644c8f59565"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8637407157326131"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_evaluator = MulticlassClassificationEvaluator()\n",
    "acc_ = multi_evaluator.evaluate(test_results, {multi_evaluator.metricName:\"accuracy\"})\n",
    "acc_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QsVlAltS80Tz"
   },
   "source": [
    "Nhận xét: Kết quả tương đối tốt, nên thử các thuật toán khác để tìm ra thuật toán tốt nhất"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GaR9z8oj80Tz"
   },
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1643031758989,
     "user": {
      "displayName": "Quỳnh Tôn",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "17019808029748888744"
     },
     "user_tz": -420
    },
    "id": "a8OdzewA80Tz"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier, LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1643031758990,
     "user": {
      "displayName": "Quỳnh Tôn",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "17019808029748888744"
     },
     "user_tz": -420
    },
    "id": "Q_cHKMVH80Tz"
   },
   "outputs": [],
   "source": [
    "lg = LogisticRegression(maxIter=20, regParam=0.3, elasticNetParam=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "executionInfo": {
     "elapsed": 45609,
     "status": "ok",
     "timestamp": 1643031804589,
     "user": {
      "displayName": "Quỳnh Tôn",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "17019808029748888744"
     },
     "user_tz": -420
    },
    "id": "vaJ6-2ve80T0"
   },
   "outputs": [],
   "source": [
    "predictor_1 = lg.fit(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "executionInfo": {
     "elapsed": 31,
     "status": "ok",
     "timestamp": 1643031804590,
     "user": {
      "displayName": "Quỳnh Tôn",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "17019808029748888744"
     },
     "user_tz": -420
    },
    "id": "WVJO144a80T0"
   },
   "outputs": [],
   "source": [
    "test_results_1 = predictor_1.transform(testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 22075,
     "status": "ok",
     "timestamp": 1643031826637,
     "user": {
      "displayName": "Quỳnh Tôn",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "17019808029748888744"
     },
     "user_tz": -420
    },
    "id": "tcJR9Qrl80T0",
    "outputId": "4f716484-985f-436b-ed86-1bff3eddcfbd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+-----+\n",
      "|label|prediction|count|\n",
      "+-----+----------+-----+\n",
      "|  2.0|       0.0|   25|\n",
      "|  1.0|       1.0| 1906|\n",
      "|  3.0|       2.0|   12|\n",
      "|  0.0|       1.0|  396|\n",
      "|  0.0|       4.0|    2|\n",
      "|  1.0|       0.0|  517|\n",
      "|  2.0|       2.0| 2801|\n",
      "|  3.0|       1.0|    1|\n",
      "|  2.0|       3.0|   48|\n",
      "|  1.0|       4.0|   14|\n",
      "|  4.0|       4.0| 3136|\n",
      "|  2.0|       4.0|   25|\n",
      "|  3.0|       4.0|    8|\n",
      "|  2.0|       1.0|   54|\n",
      "|  1.0|       2.0|  110|\n",
      "|  0.0|       0.0| 2749|\n",
      "|  1.0|       3.0|   33|\n",
      "|  0.0|       2.0|   93|\n",
      "|  4.0|       0.0|    5|\n",
      "|  3.0|       3.0| 2864|\n",
      "+-----+----------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_results_1.groupBy('label', 'prediction').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 25325,
     "status": "ok",
     "timestamp": 1643031851952,
     "user": {
      "displayName": "Quỳnh Tôn",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "17019808029748888744"
     },
     "user_tz": -420
    },
    "id": "oOHpJUek80T0",
    "outputId": "9c19044e-0e11-4193-92c5-d60f4adf1978"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of model at predicting: 0.9072929632774678\n"
     ]
    }
   ],
   "source": [
    "acc_eval = MulticlassClassificationEvaluator()\n",
    "acc_1 = acc_eval.evaluate(test_results_1)\n",
    "print(\"Accuracy of model at predicting: {}\".format(acc_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XrYi2VAX80T1"
   },
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "executionInfo": {
     "elapsed": 341,
     "status": "ok",
     "timestamp": 1643032143658,
     "user": {
      "displayName": "Quỳnh Tôn",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "17019808029748888744"
     },
     "user_tz": -420
    },
    "id": "f1oyiCkz80T1"
   },
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(labelCol=\"label\", \\\n",
    "                            featuresCol=\"features\",\\\n",
    "                            numTrees = 500, \\\n",
    "                            maxDepth = 5, \\\n",
    "                            maxBins = 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "executionInfo": {
     "elapsed": 348535,
     "status": "ok",
     "timestamp": 1643032497480,
     "user": {
      "displayName": "Quỳnh Tôn",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "17019808029748888744"
     },
     "user_tz": -420
    },
    "id": "VwkxzvHa80T1"
   },
   "outputs": [],
   "source": [
    "predictor_2 = rf.fit(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1643032497483,
     "user": {
      "displayName": "Quỳnh Tôn",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "17019808029748888744"
     },
     "user_tz": -420
    },
    "id": "f6xU-ZOt80T2"
   },
   "outputs": [],
   "source": [
    "test_results_2 = predictor_2.transform(testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 34104,
     "status": "ok",
     "timestamp": 1643032531576,
     "user": {
      "displayName": "Quỳnh Tôn",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "17019808029748888744"
     },
     "user_tz": -420
    },
    "id": "5kF4N4HS80T2",
    "outputId": "d52d3818-3792-4e23-df60-fd9cd6e668aa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+-----+\n",
      "|label|prediction|count|\n",
      "+-----+----------+-----+\n",
      "|  2.0|       0.0| 1849|\n",
      "|  3.0|       2.0|   31|\n",
      "|  4.0|       2.0|    5|\n",
      "|  0.0|       4.0|    7|\n",
      "|  1.0|       0.0| 2510|\n",
      "|  2.0|       2.0|  113|\n",
      "|  2.0|       3.0|    2|\n",
      "|  1.0|       4.0|   64|\n",
      "|  4.0|       4.0| 2864|\n",
      "|  2.0|       4.0|  989|\n",
      "|  3.0|       4.0| 1863|\n",
      "|  1.0|       2.0|    6|\n",
      "|  0.0|       0.0| 3243|\n",
      "|  0.0|       2.0|    1|\n",
      "|  4.0|       0.0|  272|\n",
      "|  3.0|       3.0|   17|\n",
      "|  3.0|       0.0|  974|\n",
      "+-----+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_results_2.groupBy('label', 'prediction').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 34328,
     "status": "ok",
     "timestamp": 1643032565897,
     "user": {
      "displayName": "Quỳnh Tôn",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "17019808029748888744"
     },
     "user_tz": -420
    },
    "id": "X6XvYYSk80T2",
    "outputId": "fb05ae46-788b-4b2e-94f3-51397843e8fd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|prediction|count|\n",
      "+----------+-----+\n",
      "|       0.0| 8848|\n",
      "|       4.0| 5787|\n",
      "|       3.0|   19|\n",
      "|       2.0|  156|\n",
      "+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_results_2.groupBy('prediction').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 39238,
     "status": "ok",
     "timestamp": 1643032605131,
     "user": {
      "displayName": "Quỳnh Tôn",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "17019808029748888744"
     },
     "user_tz": -420
    },
    "id": "xalFJaJ_80T2",
    "outputId": "c77102a5-2bd9-45c6-ae14-64a2efee9a4f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of model at predicting: 0.27052117616439586\n"
     ]
    }
   ],
   "source": [
    "acc_eval = MulticlassClassificationEvaluator()\n",
    "acc_2 = acc_eval.evaluate(test_results_2)\n",
    "print(\"Accuracy of model at predicting: {}\".format(acc_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Nhận xét: Trong các thuật toán trên, logistic regression cho kết quả tốt nhất"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lhNuZLLNYlwc"
   },
   "source": [
    "### Predict new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2872,
     "status": "ok",
     "timestamp": 1643032854442,
     "user": {
      "displayName": "Quỳnh Tôn",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "17019808029748888744"
     },
     "user_tz": -420
    },
    "id": "YNcn69w-Yk7M",
    "outputId": "c4c60699-bc3c-4153-d5c6-7c8100604b7f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+---+--------------------+--------------------+---------------+-----------------------+-------------+---------------+----------+\n",
      "|Unnamed: 0|Clothing ID|Age|               Title|         Review Text|Recommended IND|Positive Feedback Count|Division Name|Department Name|Class Name|\n",
      "+----------+-----------+---+--------------------+--------------------+---------------+-----------------------+-------------+---------------+----------+\n",
      "|         0|       1077| 53|Dress looks like ...|Dress runs small ...|              0|                     14|      General|        Dresses|   Dresses|\n",
      "|         1|        862| 66|            Cute top|Nice top. armhole...|              1|                      2|      General|           Tops|     Knits|\n",
      "|         2|       1080| 31|        Underwhelmed|Was really excite...|              0|                      1|      General|        Dresses|   Dresses|\n",
      "+----------+-----------+---+--------------------+--------------------+---------------+-----------------------+-------------+---------------+----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_new = df = pd.read_excel('Du lieu cung cap/womens-ecommerce-clothing-reviews/Womens_Clothing_E_Commerce_Reviews.xlsx', sheet_name='new_reviews')\n",
    "\n",
    "\n",
    "data_new = spark.createDataFrame(data_new)\n",
    "data_new.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "executionInfo": {
     "elapsed": 342,
     "status": "ok",
     "timestamp": 1643032993858,
     "user": {
      "displayName": "Quỳnh Tôn",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "17019808029748888744"
     },
     "user_tz": -420
    },
    "id": "_4f9Oti0YlA-"
   },
   "outputs": [],
   "source": [
    "data_prep_pipe_new = Pipeline(stages=[\n",
    "                                  tokenizer,\n",
    "                                  stopremove,\n",
    "                                  count_vec,\n",
    "                                  idf,\n",
    "                                  tokenizer1,\n",
    "                                  stopremove1,\n",
    "                                  count_vec1,\n",
    "                                  idf1,\n",
    "                                  clean_up])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "executionInfo": {
     "elapsed": 1547,
     "status": "ok",
     "timestamp": 1643033062242,
     "user": {
      "displayName": "Quỳnh Tôn",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "17019808029748888744"
     },
     "user_tz": -420
    },
    "id": "rlmC0ZErYlFf"
   },
   "outputs": [],
   "source": [
    "unlabeled_data = data_prep_pipe_new.fit(data_new).transform(data_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 378,
     "status": "ok",
     "timestamp": 1643033071371,
     "user": {
      "displayName": "Quỳnh Tôn",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "17019808029748888744"
     },
     "user_tz": -420
    },
    "id": "ONj1afoXYlKQ",
    "outputId": "62d5764c-1edf-47de-9bd8-e4b794269bd6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+---+--------------------+--------------------+---------------+-----------------------+-------------+---------------+----------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|Unnamed: 0|Clothing ID|Age|               Title|         Review Text|Recommended IND|Positive Feedback Count|Division Name|Department Name|Class Name|   token_Review Text|         stop_tokens|               c_vec|              tf_idf|         token_Title|   stop_tokens_Title|         c_vec_Title|             tf_idf1|            features|\n",
      "+----------+-----------+---+--------------------+--------------------+---------------+-----------------------+-------------+---------------+----------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|         0|       1077| 53|Dress looks like ...|Dress runs small ...|              0|                     14|      General|        Dresses|   Dresses|[dress, runs, sma...|[dress, runs, sma...|(147,[0,1,2,5,6,7...|(147,[0,1,2,5,6,7...|[dress, looks, li...|[dress, looks, li...|(13,[1,4,5,7,8,11...|(13,[1,4,5,7,8,11...|(161,[1,4,5,7,8,1...|\n",
      "|         1|        862| 66|            Cute top|Nice top. armhole...|              1|                      2|      General|           Tops|     Knits|[nice, top., armh...|[nice, top., armh...|(147,[3,4,9,12,14...|(147,[3,4,9,12,14...|         [cute, top]|         [cute, top]|(13,[0,6],[1.0,1.0])|(13,[0,6],[0.6931...|(161,[0,6,16,17,2...|\n",
      "|         2|       1080| 31|        Underwhelmed|Was really excite...|              0|                      1|      General|        Dresses|   Dresses|[was, really, exc...|[really, excited,...|(147,[0,5,6,7,8,1...|(147,[0,5,6,7,8,1...|      [underwhelmed]|      [underwhelmed]|      (13,[9],[1.0])|(13,[9],[1.098612...|(161,[9,13,18,19,...|\n",
      "|         3|        936| 35|  Absolutely perfect|If you are going ...|              0|                      9|      General|           Tops|  Sweaters|[if, you, are, go...|[going, ridiculou...|(147,[1,2,4,10,11...|(147,[1,2,4,10,11...|[absolutely, perf...|[absolutely, perf...|(13,[2,12],[1.0,1...|(13,[2,12],[1.098...|(161,[2,12,14,15,...|\n",
      "|         4|        872| 35|   Cute comfy casual|I saw this online...|              1|                      0|      General|           Tops|     Knits|[i, saw, this, on...|[saw, online, imm...|(147,[8,13,21,26,...|(147,[8,13,21,26,...|[cute, comfy, cas...|[cute, comfy, cas...|(13,[0,3,10],[1.0...|(13,[0,3,10],[0.6...|(161,[0,3,10,21,2...|\n",
      "+----------+-----------+---+--------------------+--------------------+---------------+-----------------------+-------------+---------------+----------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "unlabeled_data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "executionInfo": {
     "elapsed": 347,
     "status": "ok",
     "timestamp": 1643033120253,
     "user": {
      "displayName": "Quỳnh Tôn",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "17019808029748888744"
     },
     "user_tz": -420
    },
    "id": "bJFAFhitYlSr"
   },
   "outputs": [],
   "source": [
    "unlabeled_data = unlabeled_data.select(\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "executionInfo": {
     "elapsed": 367,
     "status": "ok",
     "timestamp": 1643033173784,
     "user": {
      "displayName": "Quỳnh Tôn",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "17019808029748888744"
     },
     "user_tz": -420
    },
    "id": "ozqhog5OZ9Ll"
   },
   "outputs": [],
   "source": [
    "prediction_new = predictor_1.transform(unlabeled_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 745,
     "status": "error",
     "timestamp": 1643033320253,
     "user": {
      "displayName": "Quỳnh Tôn",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "17019808029748888744"
     },
     "user_tz": -420
    },
    "id": "Sr-xr9_mZ9S2",
    "outputId": "3167936e-86c0-48b2-aebb-d5c59696cfec"
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-78-1d408618731e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprediction_new\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/content/spark-2.4.0-bin-hadoop2.7/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    376\u001b[0m         \"\"\"\n\u001b[1;32m    377\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 378\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    379\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/content/spark-2.4.0-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/content/spark-2.4.0-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/content/spark-2.4.0-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o1576.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 238.0 failed 1 times, most recent failure: Lost task 0.0 in stage 238.0 (TID 2514, localhost, executor driver): org.apache.spark.SparkException: Failed to execute user defined function($anonfun$1: (struct<type:tinyint,size:int,indices:array<int>,values:array<double>>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.project_doConsume_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:619)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:255)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:247)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.IllegalArgumentException: requirement failed: The columns of A don't match the number of elements of x. A: 41394, x: 161\n\tat scala.Predef$.require(Predef.scala:224)\n\tat org.apache.spark.ml.linalg.BLAS$.gemv(BLAS.scala:539)\n\tat org.apache.spark.ml.classification.LogisticRegressionModel$$anonfun$34.apply(LogisticRegression.scala:1007)\n\tat org.apache.spark.ml.classification.LogisticRegressionModel$$anonfun$34.apply(LogisticRegression.scala:1005)\n\tat org.apache.spark.ml.classification.LogisticRegressionModel.predictRaw(LogisticRegression.scala:1155)\n\tat org.apache.spark.ml.classification.LogisticRegressionModel.predictRaw(LogisticRegression.scala:930)\n\tat org.apache.spark.ml.classification.ProbabilisticClassificationModel$$anonfun$1.apply(ProbabilisticClassifier.scala:117)\n\tat org.apache.spark.ml.classification.ProbabilisticClassificationModel$$anonfun$1.apply(ProbabilisticClassifier.scala:116)\n\t... 22 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1887)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1875)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1874)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1874)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2108)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2057)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2046)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:365)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3384)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2545)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2545)\n\tat org.apache.spark.sql.Dataset$$anonfun$53.apply(Dataset.scala:3365)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3364)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2545)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2759)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:255)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:292)\n\tat sun.reflect.GeneratedMethodAccessor104.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: Failed to execute user defined function($anonfun$1: (struct<type:tinyint,size:int,indices:array<int>,values:array<double>>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.project_doConsume_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:619)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:255)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:247)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: java.lang.IllegalArgumentException: requirement failed: The columns of A don't match the number of elements of x. A: 41394, x: 161\n\tat scala.Predef$.require(Predef.scala:224)\n\tat org.apache.spark.ml.linalg.BLAS$.gemv(BLAS.scala:539)\n\tat org.apache.spark.ml.classification.LogisticRegressionModel$$anonfun$34.apply(LogisticRegression.scala:1007)\n\tat org.apache.spark.ml.classification.LogisticRegressionModel$$anonfun$34.apply(LogisticRegression.scala:1005)\n\tat org.apache.spark.ml.classification.LogisticRegressionModel.predictRaw(LogisticRegression.scala:1155)\n\tat org.apache.spark.ml.classification.LogisticRegressionModel.predictRaw(LogisticRegression.scala:930)\n\tat org.apache.spark.ml.classification.ProbabilisticClassificationModel$$anonfun$1.apply(ProbabilisticClassifier.scala:117)\n\tat org.apache.spark.ml.classification.ProbabilisticClassificationModel$$anonfun$1.apply(ProbabilisticClassifier.scala:116)\n\t... 22 more\n"
     ]
    }
   ],
   "source": [
    "prediction_new.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Cau1.2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
